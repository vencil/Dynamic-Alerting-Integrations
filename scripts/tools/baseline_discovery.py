#!/usr/bin/env python3
"""baseline_discovery.py â€” Baseline Discovery å·¥å…·ã€‚

åœ¨è² è¼‰æ³¨å…¥ç’°å¢ƒä¸‹è§€æ¸¬æŒ‡æ¨™ï¼Œå”åŠ©æ±ºå®šåˆç†çš„é–¾å€¼è¨­å®šã€‚
é€é Prometheus API æ¡é›†æŒ‡æ¨™æ™‚é–“åºåˆ—ï¼Œè¨ˆç®—çµ±è¨ˆæ‘˜è¦ï¼ˆp50/p90/p95/p99/maxï¼‰ï¼Œ
ç”¢å‡º CSV + å»ºè­°é–¾å€¼å ±å‘Šã€‚

ç”¨æ³•:
  # è§€æ¸¬ 30 åˆ†é˜ï¼Œæ¯ 30 ç§’æ¡æ¨£ä¸€æ¬¡
  python3 baseline_discovery.py \
    --tenant db-a \
    --duration 1800 --interval 30 \
    --prometheus http://localhost:9090

  # æŒ‡å®šè§€æ¸¬æŒ‡æ¨™ï¼ˆé è¨­è§€æ¸¬æ‰€æœ‰å·²çŸ¥æŒ‡æ¨™ï¼‰
  python3 baseline_discovery.py \
    --tenant db-a \
    --metrics connections,cpu,slow_queries \
    --prometheus http://localhost:9090

  # Dry-runï¼šåƒ…é¡¯ç¤ºè¦è§€æ¸¬çš„æŒ‡æ¨™ï¼Œä¸å¯¦éš›æ¡æ¨£
  python3 baseline_discovery.py \
    --tenant db-a --dry-run

  # æ­é…è² è¼‰æ³¨å…¥ä½¿ç”¨ï¼ˆå…¸å‹æµç¨‹ï¼‰ï¼š
  #   Terminal 1: ./scripts/run_load.sh --tenant db-a --type composite
  #   Terminal 2: python3 scripts/tools/baseline_discovery.py --tenant db-a

éœ€æ±‚:
  - Prometheus Query API å¯é”
  - å»ºè­°æ­é… run_load.sh è² è¼‰æ³¨å…¥åŒæ™‚ä½¿ç”¨
"""

import sys
import os
import csv
import json
import time
import math
import argparse
import urllib.request
import urllib.parse

# é è¨­è§€æ¸¬æŒ‡æ¨™ï¼šPromQL æ¨¡æ¿ (tenant æœƒè¢«æ›¿æ›)
DEFAULT_METRICS = {
    "connections": {
        "query": 'mysql_global_status_threads_connected{{tenant="{tenant}"}}',
        "unit": "connections",
        "description": "MariaDB active connections",
    },
    "cpu": {
        "query": 'rate(container_cpu_usage_seconds_total{{namespace="{tenant}",container="mariadb"}}[5m]) * 100',
        "unit": "%",
        "description": "Container CPU usage rate",
    },
    "slow_queries": {
        "query": 'rate(mysql_global_status_slow_queries{{tenant="{tenant}"}}[5m]) * 60',
        "unit": "queries/min",
        "description": "Slow queries per minute",
    },
    "memory": {
        "query": 'container_memory_working_set_bytes{{namespace="{tenant}",container="mariadb"}} / 1024 / 1024',
        "unit": "MiB",
        "description": "Container memory working set",
    },
    "disk_io": {
        "query": 'rate(container_fs_reads_bytes_total{{namespace="{tenant}",container="mariadb"}}[5m]) / 1024',
        "unit": "KiB/s",
        "description": "Disk read throughput",
    },
}


def query_prometheus(prom_url, promql):
    """Execute Prometheus instant query."""
    url = f"{prom_url}/api/v1/query"
    params = urllib.parse.urlencode({"query": promql})
    full_url = f"{url}?{params}"

    try:
        req = urllib.request.Request(full_url)
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read().decode())
    except Exception as e:
        return None, str(e)

    if data.get("status") != "success":
        return None, data.get("error", "Unknown error")

    results = data.get("data", {}).get("result", [])
    return results, None


def extract_scalar(results):
    """Extract first scalar value from Prometheus result."""
    if not results:
        return None
    val_str = results[0].get("value", [None, None])[1]
    try:
        val = float(val_str)
        if math.isnan(val) or math.isinf(val):
            return None
        return val
    except (TypeError, ValueError):
        return None


def percentile(sorted_values, p):
    """Calculate percentile from sorted list."""
    if not sorted_values:
        return None
    k = (len(sorted_values) - 1) * (p / 100.0)
    f = int(k)
    c = f + 1
    if c >= len(sorted_values):
        return sorted_values[-1]
    return sorted_values[f] + (k - f) * (sorted_values[c] - sorted_values[f])


def compute_stats(samples):
    """Compute statistics from sample list."""
    valid = [s for s in samples if s is not None]
    if not valid:
        return {
            "count": 0, "min": None, "max": None,
            "avg": None, "p50": None, "p90": None, "p95": None, "p99": None,
        }

    sorted_vals = sorted(valid)
    return {
        "count": len(valid),
        "min": sorted_vals[0],
        "max": sorted_vals[-1],
        "avg": sum(valid) / len(valid),
        "p50": percentile(sorted_vals, 50),
        "p90": percentile(sorted_vals, 90),
        "p95": percentile(sorted_vals, 95),
        "p99": percentile(sorted_vals, 99),
    }


def suggest_threshold(stats, metric_name):
    """Suggest threshold based on observed statistics.

    ç­–ç•¥ï¼š
    - warning: p95 Ã— 1.2 (æ­£å¸¸é‹è¡Œæ™‚ 95% çš„å€¼å†åŠ  20% ç·©è¡)
    - critical: p99 Ã— 1.5 (æ¥è¿‘æ¥µé™å€¼å†åŠ  50% ç·©è¡)
    - è‹¥è§€æ¸¬æ¨£æœ¬ä¸è¶³ (<10)ï¼Œä¸çµ¦å»ºè­°
    """
    if stats["count"] < 10:
        return {"warning": None, "critical": None, "note": "æ¨£æœ¬ä¸è¶³ï¼Œå»ºè­°å»¶é•·è§€æ¸¬æ™‚é–“"}

    warning = None
    critical = None

    if stats["p95"] is not None and stats["p95"] > 0:
        warning = round(stats["p95"] * 1.2, 2)
    if stats["p99"] is not None and stats["p99"] > 0:
        critical = round(stats["p99"] * 1.5, 2)

    # ç‰¹æ®Šé‚è¼¯ï¼šconnections å»ºè­°å–æ•´
    if metric_name == "connections":
        if warning is not None:
            warning = int(math.ceil(warning))
        if critical is not None:
            critical = int(math.ceil(critical))

    return {"warning": warning, "critical": critical, "note": "åŸºæ–¼ p95Ã—1.2 / p99Ã—1.5"}


def main():
    parser = argparse.ArgumentParser(
        description="Baseline Discovery â€” è² è¼‰è§€æ¸¬ + é–¾å€¼å»ºè­°å·¥å…·",
    )
    parser.add_argument("--tenant", required=True, help="Tenant namespace (e.g. db-a)")
    parser.add_argument("--prometheus", default="http://localhost:9090",
                        help="Prometheus URL (é è¨­: http://localhost:9090)")
    parser.add_argument("--duration", type=int, default=600,
                        help="è§€æ¸¬æŒçºŒæ™‚é–“ï¼ˆç§’ï¼Œé è¨­: 600 = 10 åˆ†é˜ï¼‰")
    parser.add_argument("--interval", type=int, default=15,
                        help="æ¡æ¨£é–“éš”ï¼ˆç§’ï¼Œé è¨­: 15ï¼‰")
    parser.add_argument("--metrics", default=None,
                        help="è§€æ¸¬æŒ‡æ¨™ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œé è¨­: å…¨éƒ¨ï¼‰")
    parser.add_argument("-o", "--output-dir", default="baseline_output",
                        help="è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­: baseline_outputï¼‰")
    parser.add_argument("--dry-run", action="store_true",
                        help="åƒ…é¡¯ç¤ºè¦è§€æ¸¬çš„æŒ‡æ¨™ï¼Œä¸å¯¦éš›æ¡æ¨£")

    args = parser.parse_args()

    # é¸æ“‡æŒ‡æ¨™
    if args.metrics:
        metric_keys = [m.strip() for m in args.metrics.split(",")]
    else:
        metric_keys = list(DEFAULT_METRICS.keys())

    metrics = {}
    for key in metric_keys:
        if key not in DEFAULT_METRICS:
            print(f"âš ï¸  æœªçŸ¥æŒ‡æ¨™: {key}ï¼ˆå¯ç”¨: {', '.join(DEFAULT_METRICS.keys())}ï¼‰",
                  file=sys.stderr)
            continue
        metrics[key] = DEFAULT_METRICS[key].copy()
        metrics[key]["query"] = metrics[key]["query"].format(tenant=args.tenant)

    if not metrics:
        print("éŒ¯èª¤: ç„¡æœ‰æ•ˆæŒ‡æ¨™å¯è§€æ¸¬", file=sys.stderr)
        sys.exit(1)

    # Dry-run
    if args.dry_run:
        print(f"\nğŸ“‹ Baseline Discovery â€” Dry Run")
        print(f"   Tenant: {args.tenant}")
        print(f"   Duration: {args.duration}s, Interval: {args.interval}s")
        print(f"   Samples: ~{args.duration // args.interval}")
        print(f"\nè§€æ¸¬æŒ‡æ¨™:")
        for key, info in metrics.items():
            print(f"  â€¢ {key}: {info['description']}")
            print(f"    Query: {info['query']}")
        return

    # é–‹å§‹è§€æ¸¬
    total_samples = args.duration // args.interval
    print(f"\nğŸ” Baseline Discovery â€” é–‹å§‹è§€æ¸¬")
    print(f"   Tenant: {args.tenant}")
    print(f"   Prometheus: {args.prometheus}")
    print(f"   Duration: {args.duration}s, Interval: {args.interval}s")
    print(f"   Expected samples: {total_samples}")
    print(f"   Metrics: {', '.join(metrics.keys())}")
    print()

    # æ”¶é›†æ•¸æ“š
    samples = {key: [] for key in metrics}
    timestamps = []

    for i in range(total_samples):
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        timestamps.append(ts)
        sys.stdout.write(f"\r  æ¡æ¨£ {i+1}/{total_samples} ({ts})")
        sys.stdout.flush()

        for key, info in metrics.items():
            results, err = query_prometheus(args.prometheus, info["query"])
            if err:
                samples[key].append(None)
            else:
                samples[key].append(extract_scalar(results))

        if i < total_samples - 1:
            time.sleep(args.interval)

    print(f"\n\nâœ… è§€æ¸¬å®Œæˆï¼šå…± {total_samples} å€‹æ¡æ¨£é»\n")

    # è¨ˆç®—çµ±è¨ˆ
    all_stats = {}
    for key in metrics:
        all_stats[key] = compute_stats(samples[key])

    # è¼¸å‡ºå ±å‘Š
    print(f"{'='*70}")
    print(f"ğŸ“Š Baseline Discovery Report â€” {args.tenant}")
    print(f"{'='*70}\n")

    for key, info in metrics.items():
        stats = all_stats[key]
        suggestion = suggest_threshold(stats, key)

        print(f"â–  {key} ({info['description']})")
        print(f"  Unit: {info['unit']}")

        if stats["count"] == 0:
            print(f"  âš ï¸  ç„¡æœ‰æ•ˆè³‡æ–™\n")
            continue

        print(f"  Samples: {stats['count']}")
        print(f"  Range: {stats['min']:.2f} ~ {stats['max']:.2f}")
        print(f"  Average: {stats['avg']:.2f}")
        print(f"  Percentiles: p50={stats['p50']:.2f}  p90={stats['p90']:.2f}  "
              f"p95={stats['p95']:.2f}  p99={stats['p99']:.2f}")

        if suggestion["warning"] is not None:
            print(f"  ğŸ’¡ å»ºè­° warning: {suggestion['warning']}  "
                  f"critical: {suggestion['critical']}  ({suggestion['note']})")
        else:
            print(f"  ğŸ’¡ {suggestion['note']}")
        print()

    # å¯«å…¥ CSV
    os.makedirs(args.output_dir, exist_ok=True)

    # åŸå§‹æ™‚é–“åºåˆ—
    ts_path = os.path.join(args.output_dir, f"baseline-{args.tenant}-timeseries.csv")
    with open(ts_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        header = ["timestamp"] + list(metrics.keys())
        writer.writerow(header)
        for i, ts in enumerate(timestamps):
            row = [ts] + [samples[key][i] for key in metrics]
            writer.writerow(row)
    os.chmod(ts_path, 0o600)

    # çµ±è¨ˆæ‘˜è¦ + å»ºè­°
    summary_path = os.path.join(args.output_dir, f"baseline-{args.tenant}-summary.csv")
    with open(summary_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow([
            "metric", "unit", "samples", "min", "max", "avg",
            "p50", "p90", "p95", "p99",
            "suggested_warning", "suggested_critical", "note",
        ])
        for key, info in metrics.items():
            stats = all_stats[key]
            suggestion = suggest_threshold(stats, key)
            writer.writerow([
                key, info["unit"], stats["count"],
                stats["min"], stats["max"], stats["avg"],
                stats["p50"], stats["p90"], stats["p95"], stats["p99"],
                suggestion["warning"], suggestion["critical"], suggestion["note"],
            ])
    os.chmod(summary_path, 0o600)

    print(f"ğŸ“ è¼¸å‡º:")
    print(f"  æ™‚é–“åºåˆ—: {ts_path}")
    print(f"  çµ±è¨ˆæ‘˜è¦: {summary_path}")

    # å»ºè­° patch æŒ‡ä»¤
    print(f"\n{'='*70}")
    print(f"ğŸ’¡ å»ºè­° patch æŒ‡ä»¤ï¼ˆå¯ç›´æ¥åŸ·è¡Œæˆ–èª¿æ•´å¾Œä½¿ç”¨ï¼‰:")
    print(f"{'='*70}\n")

    for key in metrics:
        suggestion = suggest_threshold(all_stats[key], key)
        if suggestion["warning"] is not None:
            config_key = f"mysql_{key}" if not key.startswith("container_") else key
            print(f"  # {key}: warning={suggestion['warning']}")
            print(f"  python3 scripts/tools/patch_config.py {args.tenant} {config_key} {suggestion['warning']}")
            if suggestion["critical"] is not None:
                print(f"  python3 scripts/tools/patch_config.py {args.tenant} {config_key}_critical {suggestion['critical']}")
            print()


if __name__ == "__main__":
    main()
