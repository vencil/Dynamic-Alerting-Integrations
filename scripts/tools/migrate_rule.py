#!/usr/bin/env python3
"""migrate_rule.py â€” å‚³çµ± Prometheus è­¦å ±è¦å‰‡é·ç§»è¼”åŠ©å·¥å…· (v4 â€” AST Engine)ã€‚

è‡ªå‹•å°‡å‚³çµ±çš„ PromQL (å¯«æ­»æ•¸å€¼) è½‰æ›ç‚ºæœ¬å°ˆæ¡ˆçš„ã€Œå‹•æ…‹å¤šç§Ÿæˆ¶ã€ä¸‰ä»¶å¥—ï¼š
1. Tenant ConfigMap YAML    â†’ migration_output/tenant-config.yaml
2. å¹³å° Recording Rule      â†’ migration_output/platform-recording-rules.yaml
3. å¹³å°å‹•æ…‹ Alert Rule      â†’ migration_output/platform-alert-rules.yaml
4. é·ç§»å ±å‘Š                 â†’ migration_output/migration-report.txt
5. Triage CSV               â†’ migration_output/triage-report.csv
6. Prefix Mapping           â†’ migration_output/prefix-mapping.yaml

ç”¨æ³•:
  python3 migrate_rule.py <legacy_rules.yml>                    # é è¨­æª”æ¡ˆè¼¸å‡º
  python3 migrate_rule.py <legacy_rules.yml> --dry-run          # åƒ…é¡¯ç¤ºå ±å‘Šï¼Œä¸ç”¢ç”Ÿæª”æ¡ˆ
  python3 migrate_rule.py <legacy_rules.yml> --interactive      # é‡åˆ°ä¸ç¢ºå®šæ™‚è©¢å•ä½¿ç”¨è€…
  python3 migrate_rule.py <legacy_rules.yml> -o /custom/path    # è‡ªè¨‚è¼¸å‡ºç›®éŒ„
  python3 migrate_rule.py <legacy_rules.yml> --triage           # Triage æ¨¡å¼: åªç”¢å‡º CSV åˆ†æ¡¶å ±å‘Š
  python3 migrate_rule.py <legacy_rules.yml> --no-prefix        # åœç”¨ custom_ å‰ç¶´ (ä¸å»ºè­°)
  python3 migrate_rule.py <legacy_rules.yml> --no-ast           # å¼·åˆ¶ä½¿ç”¨èˆŠç‰ˆ regex å¼•æ“

v4 å‡ç´š (AST Engine â€” Phase 11):
  - promql-parser (Rust/PyO3) å–ä»£ regex é€²è¡Œ metric name è¾¨è­˜
  - AST-Informed String Surgery: ç²¾æº– prefix æ›¿æ› + tenant label æ³¨å…¥
  - Reparse é©—è­‰: ç¢ºä¿æ”¹å¯«å¾Œçš„ PromQL ä»ç„¶åˆæ³•
  - Graceful degradation: promql-parser ä¸å¯ç”¨æ™‚è‡ªå‹•é™ç´šç‚º regex
"""

import sys
import re
import os
import csv
import argparse
import yaml

# ---------------------------------------------------------------------------
# AST Engine: promql-parser (optional â€” graceful degradation)
# ---------------------------------------------------------------------------
try:
    import promql_parser
    HAS_AST = True
except ImportError:
    HAS_AST = False


# ============================================================
# AST Engine: PromQL AST èµ°è¨ªèˆ‡æ”¹å¯«
# ============================================================

def _walk_vector_selectors(node):
    """éè¿´èµ°è¨ª ASTï¼Œyield æ‰€æœ‰ VectorSelector ç¯€é»ã€‚

    æ”¯æ´çš„ AST ç¯€é»é¡å‹:
      BinaryExpr (.lhs, .rhs), ParenExpr (.expr), UnaryExpr (.expr),
      AggregateExpr (.expr), Call (.args), MatrixSelector (.vector_selector),
      SubqueryExpr (.expr), VectorSelector (leaf node)
    """
    tname = type(node).__name__
    if tname == 'VectorSelector':
        yield node
        return
    if tname == 'MatrixSelector':
        vs = getattr(node, 'vector_selector', None)
        if vs is not None:
            yield vs
        return
    # Recurse into known child attributes
    for attr in ('lhs', 'rhs', 'expr'):
        child = getattr(node, attr, None)
        if child is not None:
            yield from _walk_vector_selectors(child)
    # Function call arguments
    args = getattr(node, 'args', None)
    if args:
        for arg in args:
            yield from _walk_vector_selectors(arg)
    # AggregateExpr param (e.g. histogram_quantile(0.95, ...))
    param = getattr(node, 'param', None)
    if param is not None and type(param).__name__ not in ('NumberLiteral', 'StringLiteral'):
        yield from _walk_vector_selectors(param)


def extract_metrics_ast(expr_str):
    """ä½¿ç”¨ AST ç²¾æº–æå– PromQL ä¸­æ‰€æœ‰ metric åç¨±ã€‚

    å›å‚³: list of unique metric names (ä¿ç•™å‡ºç¾é †åº)ã€‚
    è‹¥ promql-parser ä¸å¯ç”¨æˆ–è§£æå¤±æ•—ï¼Œå›å‚³ç©º list (å‘¼å«ç«¯é™ç´šç‚º regex)ã€‚
    """
    if not HAS_AST:
        return []
    try:
        ast = promql_parser.parse(expr_str)
    except Exception:
        return []
    names = []
    for vs in _walk_vector_selectors(ast):
        name = vs.name
        if name and name not in names:
            names.append(name)
    return names


def extract_label_matchers_ast(expr_str):
    """ä½¿ç”¨ AST æå–æ¯å€‹ VectorSelector çš„ label matchersã€‚

    å›å‚³: list of {"metric": str, "labels": dict}
    åªä¿ç•™ã€Œæœ‰æ„ç¾©ã€çš„ç¶­åº¦æ¨™ç±¤ (æ’é™¤ job/instance/__name__/namespace/pod/container)ã€‚
    """
    if not HAS_AST:
        return []
    try:
        ast = promql_parser.parse(expr_str)
    except Exception:
        return []

    skip_labels = frozenset({'job', 'instance', '__name__', 'namespace', 'pod', 'container'})
    results = []
    for vs in _walk_vector_selectors(ast):
        name = vs.name or ''
        matchers_obj = vs.matchers
        if matchers_obj is None:
            continue
        inner = getattr(matchers_obj, 'matchers', [])
        labels = {}
        for m in inner:
            if m.name in skip_labels:
                continue
            # Only exact-match labels for dimension hints
            if str(m.op) == 'MatchOp.Equal':
                labels[m.name] = m.value
        if labels:
            results.append({"metric": name, "labels": labels})
    return results


def detect_semantic_break_ast(expr_str):
    """ä½¿ç”¨ AST åµæ¸¬èªç¾©ä¸å¯è½‰æ›çš„å‡½å¼ (absent, predict_linear ç­‰)ã€‚

    å›å‚³: True å¦‚æœè¡¨é”å¼åŒ…å«èªç¾©ä¸­æ–·å‡½å¼ã€‚
    """
    if not HAS_AST:
        return False
    try:
        ast = promql_parser.parse(expr_str)
    except Exception:
        return False

    def _walk_calls(node):
        tname = type(node).__name__
        if tname == 'Call':
            func_obj = getattr(node, 'func', None)
            if func_obj is not None:
                fname = getattr(func_obj, 'name', '')
                if fname in SEMANTIC_BREAK_FUNCS:
                    return True
            # Walk into Call arguments (the only children of a Call node)
            args = getattr(node, 'args', [])
            if args:
                for arg in args:
                    if _walk_calls(arg):
                        return True
            return False
        # Non-Call nodes: walk known child attributes
        for attr in ('lhs', 'rhs', 'expr'):
            child = getattr(node, attr, None)
            if child is not None:
                if _walk_calls(child):
                    return True
        # AggregateExpr / other nodes with args
        args = getattr(node, 'args', None)
        if args:
            for arg in args:
                if _walk_calls(arg):
                    return True
        return False

    return _walk_calls(ast)


def rewrite_expr_prefix(expr_str, rename_map):
    """AST-Informed String Surgery: ç²¾æº–æ›¿æ› metric åç¨± (åŠ  prefix)ã€‚

    rename_map: dict {old_name: new_name}
    ä½¿ç”¨ word-boundary regexï¼Œç¢ºä¿ä¸èª¤æ”¹ label name æˆ–å­å­—ä¸²ã€‚
    æ”¹å¯«å¾Œ reparse é©—è­‰ï¼›é©—è­‰å¤±æ•—å›å‚³åŸå§‹å­—ä¸²ã€‚
    """
    result = expr_str
    for old_name, new_name in rename_map.items():
        if old_name == new_name:
            continue
        result = re.sub(r'\b' + re.escape(old_name) + r'\b', new_name, result)

    # Validate rewrite
    if HAS_AST:
        try:
            promql_parser.parse(result)
        except Exception:
            return expr_str  # é©—è­‰å¤±æ•—ï¼Œå›é€€åŸå§‹
    return result


def rewrite_expr_tenant_label(expr_str, metric_names):
    """AST-Informed String Surgery: æ³¨å…¥ tenant label matcherã€‚

    å°æ¯å€‹ metric name:
      - æœ‰ {...} çš„: åœ¨ { å¾Œæ’å…¥ tenant=~".+",
      - ç„¡ label çš„: åœ¨ metric name å¾Œé™„åŠ  {tenant=~".+"}
    æ”¹å¯«å¾Œ reparse é©—è­‰ã€‚

    Known limitation: è‹¥åŒä¸€ metric åŒæ™‚æœ‰å¸¶ label å’Œè£¸éœ²çš„ç”¨æ³• (e.g.
    "my_metric > on() group_left my_metric{a="1"}")ï¼Œif/else åˆ†æ”¯åªæœƒå¥—ç”¨
    å¸¶ label çš„ patternï¼Œè£¸éœ²å‡ºç¾ä¸æœƒè¢«æ³¨å…¥ tenantã€‚Recording rule LHS é€šå¸¸
    åªæœ‰ä¸€ç¨®å½¢å¼ï¼Œæ­¤é™åˆ¶åœ¨å¯¦éš›é·ç§»å ´æ™¯ä¸­å½±éŸ¿æ¥µå°ã€‚
    """
    result = expr_str
    for name in metric_names:
        # Pattern 1: metric{existing...} â†’ metric{tenant=~".+",existing...}
        pattern_with_labels = r'\b' + re.escape(name) + r'\{'
        if re.search(pattern_with_labels, result):
            result = re.sub(
                pattern_with_labels,
                name + '{tenant=~".+",',
                result
            )
        else:
            # Pattern 2: bare metric name â†’ metric{tenant=~".+"}
            # Negative lookahead: not followed by { or alphanumeric (substring)
            pattern_bare = r'\b' + re.escape(name) + r'(?![{a-zA-Z0-9_])'
            result = re.sub(pattern_bare, name + '{tenant=~".+"}', result)

    # Validate rewrite
    if HAS_AST:
        try:
            promql_parser.parse(result)
        except Exception:
            return expr_str  # é©—è­‰å¤±æ•—ï¼Œå›é€€åŸå§‹
    return result


# ============================================================
# Metric Dictionary: è¼‰å…¥å¤–éƒ¨å•Ÿç™¼å¼å­—å…¸
# ============================================================

def load_metric_dictionary(script_dir=None):
    """è¼‰å…¥ metric-dictionary.yaml å•Ÿç™¼å¼å­—å…¸ã€‚"""
    if script_dir is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
    dict_path = os.path.join(script_dir, "metric-dictionary.yaml")
    if os.path.exists(dict_path):
        with open(dict_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f) or {}
    return {}


# ============================================================
# Heuristics: æ™ºèƒ½èšåˆçŒœæ¸¬
# ============================================================

def guess_aggregation(base_key, expr_str):
    """æ ¹æ“š metric åç¨±å’Œ PromQL è¡¨é”å¼æ™ºèƒ½çŒœæ¸¬èšåˆæ¨¡å¼ã€‚

    å›å‚³: (mode, reason) â€” mode ç‚º "sum" æˆ– "max"ï¼Œreason ç‚ºæ¨ç†èªªæ˜ã€‚
    """
    expr_lower = expr_str.lower()
    key_lower = base_key.lower()

    # Rule 1: rate() / increase() / irate() â†’ sum (å¢é›†ç¸½é‡)
    if re.search(r'\b(rate|increase|irate)\s*\(', expr_lower):
        return "sum", "åŒ…å« rate/increase â€” å¢é›†èšåˆç¸½é‡"

    # Rule 2: _total å¾Œç¶´ (Prometheus counter å‘½åæ…£ä¾‹) â†’ sum
    if key_lower.endswith('_total'):
        return "sum", "Counter å‘½åæ…£ä¾‹ (_total) â€” å¢é›†èšåˆç¸½é‡"

    # Rule 3: åŒ…å«ç™¾åˆ†æ¯”/æ¯”ç‡/å»¶é²/è½å¾Œ â†’ max (æœ€å¼±ç’°ç¯€)
    ratio_keywords = ('percent', 'ratio', 'lag', 'latency', 'delay',
                      'utilization', 'usage', 'saturation')
    for kw in ratio_keywords:
        if kw in key_lower:
            return "max", f"é—œéµå­— '{kw}' â€” æœ€å¼±ç’°ç¯€ (å–®é»ç“¶é ¸)"

    # Rule 4: åŒ…å« total/bytes/count â†’ sum (ç´¯ç©é‡)
    sum_keywords = ('total', 'bytes', 'count', 'size', 'sent', 'received',
                    'evicted', 'expired', 'rejected', 'errors', 'requests')
    for kw in sum_keywords:
        if kw in key_lower:
            return "sum", f"é—œéµå­— '{kw}' â€” å¢é›†ç´¯ç©é‡"

    # Rule 5: åŒ…å«é™¤æ³• â†’ max (é€šå¸¸æ˜¯ ratio/percent è¨ˆç®—)
    if '/' in expr_str:
        return "max", "åŒ…å«é™¤æ³•é‹ç®— â€” é€šå¸¸ç‚ºæ¯”ç‡è¨ˆç®—"

    # Rule 6: é€£ç·šæ•¸ã€ä½‡åˆ—é•·åº¦ç­‰ â†’ max (å–®é»ä¸Šé™)
    max_keywords = ('connections', 'connected', 'clients', 'threads',
                    'queue', 'replication', 'slave', 'replica')
    for kw in max_keywords:
        if kw in key_lower:
            return "max", f"é—œéµå­— '{kw}' â€” å–®é»ä¸Šé™"

    # Fallback â†’ max (ä¿éšœå–®é»å®‰å…¨)
    return "max", "é è¨­ Fallback â€” ä¿éšœå–®é»å®‰å…¨"


# ============================================================
# PromQL è§£æå™¨
# ============================================================

# èªç¾©ä¸å¯è½‰æ›çš„ PromQL å‡½å¼
SEMANTIC_BREAK_FUNCS = frozenset({
    'absent', 'absent_over_time', 'vector', 'scalar',
    'predict_linear', 'holt_winters', 'label_replace', 'label_join'
})

# PromQL å…§å»ºå‡½å¼/é—œéµå­— (ç”¨æ–¼è·³éï¼Œæ‰¾åˆ°çœŸæ­£çš„ metric åç¨±)
PROMQL_FUNCS = frozenset({
    'abs', 'absent', 'absent_over_time', 'avg', 'avg_over_time',
    'ceil', 'changes', 'clamp', 'clamp_max', 'clamp_min', 'count',
    'count_over_time', 'day_of_month', 'day_of_week', 'days_in_month',
    'delta', 'deriv', 'exp', 'floor', 'group', 'histogram_quantile',
    'holt_winters', 'hour', 'idelta', 'increase', 'irate', 'label_join',
    'label_replace', 'last_over_time', 'ln', 'log2', 'log10', 'max',
    'max_over_time', 'min', 'min_over_time', 'minute', 'month',
    'predict_linear', 'quantile', 'quantile_over_time', 'rate', 'resets',
    'round', 'scalar', 'sgn', 'sort', 'sort_desc', 'sqrt', 'stddev',
    'stddev_over_time', 'stdvar', 'stdvar_over_time', 'sum',
    'sum_over_time', 'time', 'timestamp', 'vector', 'year',
    'by', 'without', 'on', 'ignoring', 'group_left', 'group_right', 'bool',
})


def extract_label_matchers(expr_str):
    """å¾ PromQL è¡¨é”å¼ä¸­æå– label matchersã€‚

    v4: å„ªå…ˆä½¿ç”¨ AST å¼•æ“ï¼Œé™ç´šç‚º regexã€‚
    å›å‚³: list of dictï¼Œæ¯å€‹ dict = {"metric": str, "labels": dict}
    """
    # AST path
    ast_result = extract_label_matchers_ast(expr_str)
    if ast_result:
        return ast_result

    # Regex fallback (legacy)
    results = []
    pattern = re.compile(r'([a-zA-Z_][a-zA-Z0-9_]*)\{([^}]+)\}')
    for m in pattern.finditer(expr_str):
        metric = m.group(1)
        labels_str = m.group(2)
        labels = {}
        for pair in re.finditer(r'([a-zA-Z_][a-zA-Z0-9_]*)\s*(!?=~?)\s*"([^"]*)"', labels_str):
            lk, op, lv = pair.group(1), pair.group(2), pair.group(3)
            if lk in ('job', 'instance', '__name__', 'namespace', 'pod', 'container'):
                continue
            if op != '=':
                continue
            labels[lk] = lv
        if labels:
            results.append({"metric": metric, "labels": labels})
    return results


def extract_all_metrics(expr_str):
    """å¾ PromQL ä¸­æå–æ‰€æœ‰ metric åç¨±ã€‚

    v4: å„ªå…ˆä½¿ç”¨ AST å¼•æ“ï¼Œé™ç´šç‚º regex + function blacklistã€‚
    """
    # AST path
    ast_result = extract_metrics_ast(expr_str)
    if ast_result:
        return ast_result

    # Regex fallback (legacy)
    metrics = []
    for m in re.finditer(r'([a-zA-Z_][a-zA-Z0-9_]*)', expr_str):
        name = m.group(1)
        if name not in PROMQL_FUNCS and not name.isdigit():
            metrics.append(name)
    return list(dict.fromkeys(metrics))  # deduplicate, preserve order


def parse_expr(expr_str, use_ast=True):
    """è§£æ PromQL è¡¨é”å¼ï¼Œå˜—è©¦åˆ‡åˆ†ç‚º LHS, Operator, RHS (é–¾å€¼æ•¸å€¼)ã€‚

    v4: ä½¿ç”¨ AST é€²è¡Œ metric name è¾¨è­˜èˆ‡èªç¾©ä¸­æ–·åµæ¸¬ã€‚
    """
    match = re.match(
        r'^\s*(.*?)\s*(==|!=|>=|<=|>|<)\s*([0-9.]+(?:[eE][+-]?[0-9]+)?)\s*$',
        expr_str
    )
    if not match:
        return None

    lhs, op, rhs = match.groups()

    # èªç¾©ä¸å¯è½‰æ›çš„å‡½å¼åµæ¸¬
    if use_ast and HAS_AST and detect_semantic_break_ast(lhs):
        return None
    elif not use_ast or not HAS_AST:
        # Regex fallback â€” when AST is disabled or unavailable
        first_func = re.match(r'\s*([a-zA-Z_]+)\s*\(', lhs)
        if first_func and first_func.group(1) in SEMANTIC_BREAK_FUNCS:
            return None

    is_complex = bool(re.search(r'[\(\)\[\]/+\-*]', lhs))

    # v4: AST-based metric extraction (ç²¾æº–ï¼Œä¸éœ€ function blacklist)
    base_key = "unknown_metric"
    ast_metrics = extract_metrics_ast(lhs) if use_ast else []
    if ast_metrics:
        base_key = ast_metrics[0]

    # Regex fallback
    if base_key == "unknown_metric":
        for m in re.finditer(r'([a-zA-Z_][a-zA-Z0-9_]*)', lhs):
            if m.group(1) not in PROMQL_FUNCS:
                base_key = m.group(1)
                break

    return {
        "lhs": lhs.strip(),
        "op": op,
        "val": rhs,
        "is_complex": is_complex,
        "base_key": base_key,
        "all_metrics": ast_metrics,
    }


# ============================================================
# Rule è™•ç†æ ¸å¿ƒ
# ============================================================

class MigrationResult:
    """å–®æ¢è¦å‰‡çš„é·ç§»çµæœã€‚"""

    def __init__(self, alert_name, status, severity="warning"):
        self.alert_name = alert_name
        self.status = status  # "perfect" | "complex" | "unparseable"
        self.severity = severity

        # ä¸‰ä»¶å¥—å…§å®¹
        self.tenant_config = {}       # {metric_key: value}
        self.recording_rules = []     # list of dict (YAML-ready)
        self.alert_rules = []         # list of dict (YAML-ready)

        # Auto-suppression ç”¨
        self.op = None                # æ¯”è¼ƒé‹ç®—å­ (e.g., ">", "<")

        # å ±å‘Šé™„åŠ è³‡è¨Š
        self.agg_mode = None
        self.agg_reason = None
        self.dim_hints = []
        self.llm_prompt = None
        self.notes = []

        # v3: Dictionary match
        self.dict_match = None        # dict entry from metric-dictionary.yaml
        self.triage_action = None     # "auto" | "review" | "skip" | "use_golden"
        self.original_expr = ""


def lookup_dictionary(metric_name, dictionary):
    """åœ¨å•Ÿç™¼å¼å­—å…¸ä¸­æŸ¥æ‰¾ metricï¼Œå›å‚³åŒ¹é…æ¢ç›®æˆ– Noneã€‚"""
    if not dictionary:
        return None
    return dictionary.get(metric_name)


def process_rule(rule, interactive=False, prefix="custom_", dictionary=None,
                  use_ast=True):
    """è™•ç†å–®æ¢å‚³çµ± Prometheus è¦å‰‡ï¼Œå›å‚³ MigrationResultã€‚

    v4: use_ast=True å•Ÿç”¨ AST å¼•æ“é€²è¡Œç²¾æº– metric è¾¨è­˜èˆ‡è¡¨é”å¼æ”¹å¯«ã€‚
    """
    alert_name = rule.get('alert')
    if not alert_name:
        return None

    expr = rule.get('expr', '')
    severity = rule.get('labels', {}).get('severity', 'warning')
    parsed = parse_expr(expr, use_ast=use_ast)

    # æƒ…å¢ƒ 3: ç„¡æ³•è§£æ
    if not parsed:
        result = MigrationResult(alert_name, "unparseable", severity)
        result.original_expr = expr
        result.triage_action = "skip"
        result.llm_prompt = (
            f"è«‹å°‡ä»¥ä¸‹å‚³çµ± Prometheus Alert è½‰æ›ç‚ºæœ¬å°ˆæ¡ˆçš„å‹•æ…‹å¤šç§Ÿæˆ¶æ¶æ§‹ï¼š\n"
            f"è¦æ±‚ï¼š\n"
            f"1. æå–é–¾å€¼ä¸¦æä¾› threshold-config.yaml ç¯„ä¾‹ã€‚\n"
            f"2. æä¾›åŒ…å« sum/max by(tenant) çš„ Recording Ruleã€‚\n"
            f"3. æä¾›å¥—ç”¨ group_left èˆ‡ unless maintenance é‚è¼¯çš„ Alert Ruleã€‚\n"
            f"4. å¦‚æœ‰ç¶­åº¦æ¨™ç±¤ (å¦‚ queue, db, index)ï¼Œè«‹ç”¨ \"metric{{label=\\\"value\\\"}}\" èªæ³•æä¾›ç¯„ä¾‹ã€‚\n\n"
            f"åŸå§‹è¦å‰‡ï¼š\n{yaml.dump([rule], sort_keys=False)}"
        )
        # Check dictionary for guidance even on unparseable
        all_metrics = extract_all_metrics(expr)
        for m in all_metrics:
            match = lookup_dictionary(m, dictionary)
            if match and match.get("golden_rule"):
                result.dict_match = match
                result.triage_action = "use_golden"
                result.notes.append(
                    f"å­—å…¸å»ºè­°: {m} â†’ é»ƒé‡‘æ¨™æº– {match['golden_rule']} ({match.get('note', '')})"
                )
                break
        return result

    metric_key = parsed["base_key"]

    # v3: Dictionary lookup
    dict_match = lookup_dictionary(metric_key, dictionary)
    has_golden = dict_match and dict_match.get("golden_rule")

    # æ±ºå®š metric key (å« prefix èˆ‡ severity)
    if prefix and not has_golden:
        prefixed_key = f"{prefix}{metric_key}"
    else:
        prefixed_key = metric_key

    metric_key_yaml = f"{prefixed_key}_critical" if severity == "critical" else prefixed_key

    # æ™ºèƒ½çŒœæ¸¬èšåˆæ¨¡å¼
    agg_mode, agg_reason = guess_aggregation(parsed["base_key"], parsed["lhs"])

    # äº’å‹•æ¨¡å¼: è¤‡é›œè¡¨é”å¼æ™‚è©¢å•ä½¿ç”¨è€…
    if interactive and parsed["is_complex"]:
        print(f"\nğŸ” Alert: {alert_name}")
        print(f"   Expr: {expr}")
        print(f"   ğŸ¤– AI çŒœæ¸¬: {agg_mode} ({agg_reason})")
        if has_golden:
            print(f"   ğŸ“– å­—å…¸å»ºè­°: æ”¹ç”¨é»ƒé‡‘æ¨™æº– {dict_match['golden_rule']}")
        choice = input(f"   é¸æ“‡èšåˆæ¨¡å¼ [s=sum / m=max / Enter=æ¡ç”¨çŒœæ¸¬]: ").strip().lower()
        if choice == 's':
            agg_mode = "sum"
            agg_reason = "ä½¿ç”¨è€…æ‰‹å‹•é¸æ“‡"
        elif choice == 'm':
            agg_mode = "max"
            agg_reason = "ä½¿ç”¨è€…æ‰‹å‹•é¸æ“‡"

    status = "complex" if parsed["is_complex"] else "perfect"
    result = MigrationResult(alert_name, status, severity)
    result.op = parsed['op']
    result.agg_mode = agg_mode
    result.agg_reason = agg_reason
    result.original_expr = expr
    result.dict_match = dict_match

    # Triage action
    if has_golden:
        result.triage_action = "use_golden"
        result.notes.append(
            f"å­—å…¸å»ºè­°: {metric_key} â†’ é»ƒé‡‘æ¨™æº– {dict_match['golden_rule']} "
            f"(Rule Pack: {dict_match.get('rule_pack', 'unknown')})"
        )
    elif status == "perfect":
        result.triage_action = "auto"
    else:
        result.triage_action = "review"

    # ç¶­åº¦æ¨™ç±¤æç¤º
    result.dim_hints = extract_label_matchers(expr)

    # === ç”¢å‡º 1. Tenant Config ===
    result.tenant_config[metric_key_yaml] = parsed['val']

    # === ç”¢å‡º 2. Recording Rules ===
    record_name = f"tenant:{prefixed_key}:{agg_mode}"
    threshold_suffix = "_critical" if severity == "critical" else ""
    threshold_name = f"tenant:alert_threshold:{prefixed_key}{threshold_suffix}"

    # v4: AST-Informed String Surgery â€” æ”¹å¯« LHS è¡¨é”å¼
    recording_lhs = parsed['lhs']
    if use_ast and HAS_AST:
        all_metrics = parsed.get('all_metrics', [])
        # Step 1: Prefix injection (å¦‚æœéœ€è¦)
        if prefix and not has_golden and all_metrics:
            rename_map = {}
            for m_name in all_metrics:
                if not m_name.startswith(prefix):
                    rename_map[m_name] = f"{prefix}{m_name}"
            if rename_map:
                recording_lhs = rewrite_expr_prefix(recording_lhs, rename_map)
        # Step 2: Tenant label injection
        rewritten_metrics = extract_metrics_ast(recording_lhs) or all_metrics
        if rewritten_metrics:
            recording_lhs = rewrite_expr_tenant_label(recording_lhs, rewritten_metrics)

    result.recording_rules.append({
        "record": record_name,
        "expr": f"{agg_mode} by(tenant) ({recording_lhs})",
    })
    result.recording_rules.append({
        "record": threshold_name,
        "expr": f'max by(tenant) (user_threshold{{metric="{prefixed_key}", severity="{severity}"}})',
    })

    # === ç”¢å‡º 3. Alert Rule ===
    alert_prefix = f"Custom" if prefix else ""
    alert_rule = {
        "alert": f"{alert_prefix}{alert_name}" if prefix else alert_name,
        "expr": (
            f"(\n"
            f"  {record_name}\n"
            f"  {parsed['op']} on(tenant) group_left\n"
            f"  {threshold_name}\n"
            f")\n"
            f'unless on(tenant) (user_state_filter{{filter="maintenance"}} == 1)'
        ),
    }
    if 'for' in rule:
        alert_rule['for'] = rule['for']
    labels = dict(rule.get('labels', {}))
    if prefix:
        labels['source'] = 'legacy'
        labels['migration_status'] = 'shadow'
    if labels:
        alert_rule['labels'] = labels
    if 'annotations' in rule:
        alert_rule['annotations'] = rule['annotations']
    result.alert_rules.append(alert_rule)

    return result


# ============================================================
# Auto-Suppression: Warning â†” Critical é…å°
# ============================================================

def apply_auto_suppression(results):
    """é…å° warning/critical è¦å‰‡ï¼Œç‚º warning æ³¨å…¥ç¬¬äºŒå±¤ unless (auto-suppression)ã€‚

    ç•¶åŒä¸€ base metric key åŒæ™‚æœ‰ warning å’Œ critical çµæœæ™‚ï¼Œwarning alert
    çš„ expr æœƒè‡ªå‹•è¿½åŠ  unless å­å¥ï¼Œç¢ºä¿ critical è§¸ç™¼æ™‚æŠ‘åˆ¶ warningã€‚

    ä¿®æ”¹ results ä¸­ warning MigrationResult çš„ alert_rules[0]["expr"]ï¼ˆin-placeï¼‰ã€‚
    å›å‚³é…å°æˆåŠŸçš„æ•¸é‡ã€‚
    """
    # å»ºç«‹ base_key â†’ {severity: result} æ˜ å°„
    pairs = {}  # base_key â†’ {"warning": result, "critical": result}
    for r in results:
        if r.status == "unparseable" or r.triage_action == "use_golden":
            continue
        if not r.tenant_config:
            continue

        # å–å‡º metric_key_yaml (tenant_config çš„ç¬¬ä¸€å€‹ key)
        metric_key_yaml = list(r.tenant_config.keys())[0]

        # æ¨å° base_keyï¼šcritical å»æ‰ _critical å¾Œç¶´
        if r.severity == "critical" and metric_key_yaml.endswith("_critical"):
            base_key = metric_key_yaml[: -len("_critical")]
        else:
            base_key = metric_key_yaml

        if base_key not in pairs:
            pairs[base_key] = {}
        pairs[base_key][r.severity] = r

    paired = 0
    for base_key, sev_map in pairs.items():
        warn_r = sev_map.get("warning")
        crit_r = sev_map.get("critical")
        if not warn_r or not crit_r:
            continue
        if not warn_r.alert_rules or len(crit_r.recording_rules) < 2:
            continue

        # å–å¾— warning çš„ data recording rule name (ç¬¬ä¸€æ¢)
        record_name = warn_r.recording_rules[0]["record"]
        # å–å¾— critical çš„ threshold recording rule name (ç¬¬äºŒæ¢)
        crit_threshold = crit_r.recording_rules[1]["record"]
        # é‹ç®—å­å–è‡ª warning result
        op = warn_r.op or ">"

        suppression_clause = (
            f"\nunless on(tenant)\n"
            f"(\n"
            f"  {record_name}\n"
            f"  {op} on(tenant) group_left\n"
            f"  {crit_threshold}\n"
            f")"
        )

        # ä¿®æ”¹ warning alert çš„ exprï¼ˆin-placeï¼‰
        for ar in warn_r.alert_rules:
            ar["expr"] += suppression_clause

        warn_r.notes.append(
            f"Auto-Suppression: å·²é…å° critical ({crit_r.alert_name})ï¼Œ"
            f"warning è§¸ç™¼æ™‚è‹¥åŒæ™‚è¶…é critical é–¾å€¼å‰‡è‡ªå‹•æŠ‘åˆ¶"
        )
        paired += 1

    return paired


# ============================================================
# v3: Triage Mode â€” CSV å ±å‘Š
# ============================================================

def write_triage_csv(results, output_dir, dictionary):
    """ç”¢å‡º CSV åˆ†æ¡¶å ±å‘Šï¼Œä¾›å¤§è¦æ¨¡é·ç§»æ™‚åœ¨ Excel ä¸­æ‰¹æ¬¡æ±ºç­–ã€‚"""
    csv_path = os.path.join(output_dir, "triage-report.csv")
    with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow([
            "Alert Name",
            "Triage Action",
            "Status",
            "Severity",
            "Metric Key",
            "Threshold",
            "Aggregation",
            "Aggregation Reason",
            "Golden Standard Match",
            "Golden Rule",
            "Rule Pack",
            "Dictionary Note",
            "Dimensions",
            "Original Expression",
        ])
        for r in results:
            golden_match = r.dict_match.get("maps_to", "") if r.dict_match else ""
            golden_rule = r.dict_match.get("golden_rule", "") if r.dict_match else ""
            rule_pack = r.dict_match.get("rule_pack", "") if r.dict_match else ""
            dict_note = r.dict_match.get("note", "") if r.dict_match else ""
            metric_keys = ", ".join(r.tenant_config.keys()) if r.tenant_config else ""
            thresholds = ", ".join(r.tenant_config.values()) if r.tenant_config else ""
            dims = "; ".join(str(d) for d in r.dim_hints) if r.dim_hints else ""

            writer.writerow([
                r.alert_name,
                r.triage_action or "unknown",
                r.status,
                r.severity,
                metric_keys,
                thresholds,
                r.agg_mode or "",
                r.agg_reason or "",
                golden_match,
                golden_rule,
                rule_pack,
                dict_note,
                dims,
                r.original_expr[:200],  # Truncate long exprs
            ])
    os.chmod(csv_path, 0o600)
    return csv_path


def write_prefix_mapping(results, output_dir, prefix):
    """ç”¢å‡º prefix mapping tableï¼Œè¨˜éŒ„ custom_ å‰ç¶´å°æ‡‰é»ƒé‡‘æ¨™æº–çš„é—œä¿‚ã€‚"""
    if not prefix:
        return None

    mapping = {}
    for r in results:
        if r.status == "unparseable":
            continue
        for key in r.tenant_config.keys():
            original = key.replace(prefix, "", 1) if key.startswith(prefix) else key
            mapping[key] = {
                "original_metric": original,
                "alert_name": r.alert_name,
                "golden_match": r.dict_match.get("maps_to") if r.dict_match else None,
                "golden_rule": r.dict_match.get("golden_rule") if r.dict_match else None,
            }

    if not mapping:
        return None

    mapping_path = os.path.join(output_dir, "prefix-mapping.yaml")
    with open(mapping_path, 'w', encoding='utf-8') as f:
        f.write("# ============================================================\n")
        f.write("# Prefix Mapping Table â€” custom_ å‰ç¶´å°æ‡‰é—œä¿‚\n")
        f.write("# ============================================================\n")
        f.write("# ç”¨é€”: æœªä¾†æ”¶æ–‚è‡³é»ƒé‡‘æ¨™æº–æ™‚çš„å°ç…§è¡¨\n")
        f.write(f"# Prefix: {prefix}\n")
        f.write("# ============================================================\n\n")
        yaml.safe_dump(mapping, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
    os.chmod(mapping_path, 0o600)
    return mapping_path


# ============================================================
# è¼¸å‡ºå¼•æ“
# ============================================================

def write_outputs(results, output_dir, prefix="custom_", dictionary=None):
    """å°‡é·ç§»çµæœå¯«å…¥åˆ†é›¢çš„ YAML æª”æ¡ˆ (å«åˆæ³• YAML çµæ§‹)ã€‚"""
    os.makedirs(output_dir, exist_ok=True)

    # --- tenant-config.yaml (å« boilerplate ç¯„ä¾‹) ---
    tenant_configs = {}
    for r in results:
        if r.status == "unparseable":
            continue
        if r.triage_action == "use_golden":
            continue  # å»ºè­°ä½¿ç”¨é»ƒé‡‘æ¨™æº–çš„ä¸è¼¸å‡ºåˆ° tenant config
        for k, v in r.tenant_config.items():
            tenant_configs[k] = v

    tenant_config_path = os.path.join(output_dir, "tenant-config.yaml")
    with open(tenant_config_path, 'w', encoding='utf-8') as f:
        f.write("# ============================================================\n")
        f.write("# Tenant Config â€” è¤‡è£½åˆ° conf.d/<tenant>.yaml\n")
        f.write("# ============================================================\n")
        f.write("# è«‹å°‡ä»¥ä¸‹å…§å®¹ç¸®æ’ä¸¦è²¼å…¥æ‚¨å°ˆå±¬çš„ tenant è¨­å®šä¸­ï¼Œä¾‹å¦‚ï¼š\n")
        f.write("# tenants:\n")
        f.write("#   my-tenant-name:\n")
        for k, v in tenant_configs.items():
            f.write(f'#     {k}: "{v}"\n')
        f.write("\n")
        for r in results:
            if r.status == "unparseable" or r.triage_action == "use_golden":
                continue
            f.write(f"# --- From: {r.alert_name} (severity: {r.severity}) ---\n")
            if r.notes:
                for note in r.notes:
                    f.write(f"# ğŸ“– {note}\n")
            for k, v in r.tenant_config.items():
                f.write(f'{k}: "{v}"\n')
            if r.dim_hints:
                f.write("# ç¶­åº¦æ¨™ç±¤æ›¿ä»£èªæ³•:\n")
                for hint in r.dim_hints:
                    label_pairs = ', '.join(f'{lk}="{lv}"' for lk, lv in hint["labels"].items())
                    dim_key = f'{list(r.tenant_config.keys())[0].split("_critical")[0]}{{{label_pairs}}}'
                    f.write(f'# "{dim_key}": "{list(r.tenant_config.values())[0]}"\n')
            f.write("\n")
    os.chmod(tenant_config_path, 0o600)

    # --- platform-recording-rules.yaml (åˆæ³• YAML, å« groups/rules çµæ§‹) ---
    # Deduplication: è¿½è¹¤å·²ç”¢å‡ºçš„ recording rule record åç¨±
    seen_records = set()
    deduplicated_rules = []
    for r in results:
        if r.status == "unparseable" or r.triage_action == "use_golden":
            continue
        for rr in r.recording_rules:
            record_name = rr["record"]
            if record_name in seen_records:
                continue
            seen_records.add(record_name)
            deduplicated_rules.append((r, rr))

    # è¨ˆç®—æ”¶æ–‚ç‡
    total_input = len([r for r in results if r.status != "unparseable"])
    total_output = len(deduplicated_rules)

    group_name = f"{prefix}migrated-recording-rules" if prefix else "migrated-recording-rules"
    recording_rules_path = os.path.join(output_dir, "platform-recording-rules.yaml")
    with open(recording_rules_path, 'w', encoding='utf-8') as f:
        f.write("# ============================================================\n")
        f.write("# Platform Recording Rules â€” å¯ç›´æ¥åˆä½µè‡³ Prometheus ConfigMap\n")
        f.write("# ============================================================\n")
        if total_input > 0:
            compression = round((1 - total_output / max(total_input * 2, 1)) * 100, 1)
            f.write(f"# æ”¶æ–‚ç‡: {total_input} æ¢è¦å‰‡ â†’ {total_output} æ¢ Recording Rules")
            f.write(f" (å£“ç¸® {compression}%)\n")
        f.write("# ============================================================\n\n")
        f.write("groups:\n")
        f.write(f"  - name: {group_name}\n")
        f.write("    rules:\n")
        for r, rr in deduplicated_rules:
            # ç•¶èšåˆæ¨¡å¼ç‚º AI çŒœæ¸¬ (éä½¿ç”¨è€…æ‰‹å‹•é¸æ“‡) æ™‚ï¼Œæ’å…¥é†’ç›®è­¦å‘Šæ–¹å¡Š
            if r.status == "complex" and r.agg_reason != "ä½¿ç”¨è€…æ‰‹å‹•é¸æ“‡":
                f.write("      # ============================================================\n")
                f.write("      # ğŸš¨ğŸš¨ğŸš¨ [AI æ™ºèƒ½çŒœæ¸¬æ³¨æ„] ğŸš¨ğŸš¨ğŸš¨\n")
                f.write("      # ============================================================\n")
                f.write(f"      # ä»¥ä¸‹ recording rule çš„èšåˆæ¨¡å¼ç‚º AI è‡ªå‹•çŒœæ¸¬: {r.agg_mode}\n")
                f.write(f"      # çŒœæ¸¬åŸå› : {r.agg_reason}\n")
                f.write(f"      # åŸå§‹ Alert: {r.alert_name}\n")
                f.write("      #\n")
                f.write("      # âš ï¸  è«‹åœ¨è¤‡è£½è²¼ä¸Šå‰ç¢ºèª:\n")
                f.write(f"      #   - èšåˆæ¨¡å¼ {r.agg_mode} æ˜¯å¦æ­£ç¢º? (sum=å¢é›†ç¸½é‡, max=å–®é»ç“¶é ¸)\n")
                f.write("      #   - å¦‚ä¸ç¢ºå®šï¼Œè«‹ç”¨ --interactive æ¨¡å¼é‡æ–°åŸ·è¡Œ\n")
                f.write("      # ============================================================\n")
            else:
                f.write(f"      # {r.alert_name} | {r.agg_mode} â€” {r.agg_reason}\n")
            f.write(f"      - record: {rr['record']}\n")
            f.write(f"        expr: {rr['expr']}\n")
            f.write("\n")
    os.chmod(recording_rules_path, 0o600)

    # --- platform-alert-rules.yaml (åˆæ³• YAML, å« groups/rules çµæ§‹) ---
    alert_group_name = f"{prefix}migrated-alert-rules" if prefix else "migrated-alert-rules"
    alert_rules_path = os.path.join(output_dir, "platform-alert-rules.yaml")
    with open(alert_rules_path, 'w', encoding='utf-8') as f:
        f.write("# ============================================================\n")
        f.write("# Platform Dynamic Alert Rules â€” å¯ç›´æ¥åˆä½µè‡³ Prometheus ConfigMap\n")
        f.write("# ============================================================\n")
        f.write("groups:\n")
        f.write(f"  - name: {alert_group_name}\n")
        f.write("    rules:\n")
        for r in results:
            if r.status == "unparseable" or r.triage_action == "use_golden":
                continue
            f.write(f"      # --- {r.alert_name} ---\n")
            # Write alert rule with proper indentation
            for ar in r.alert_rules:
                f.write(f"      - alert: {ar['alert']}\n")
                # Multiline expr â€” use YAML literal block
                f.write(f"        expr: |\n")
                for line in ar['expr'].strip().split('\n'):
                    f.write(f"          {line}\n")
                if 'for' in ar:
                    f.write(f"        for: {ar['for']}\n")
                if 'labels' in ar:
                    f.write(f"        labels:\n")
                    for lk, lv in ar['labels'].items():
                        f.write(f"          {lk}: {lv}\n")
                if 'annotations' in ar:
                    f.write(f"        annotations:\n")
                    for ak, av in ar['annotations'].items():
                        f.write(f"          {ak}: \"{av}\"\n")
            f.write("\n")
    os.chmod(alert_rules_path, 0o600)

    # --- migration-report.txt ---
    perfect = [r for r in results if r.status == "perfect"]
    complex_rules = [r for r in results if r.status == "complex"]
    unparseable = [r for r in results if r.status == "unparseable"]
    golden_matches = [r for r in results if r.triage_action == "use_golden"]

    report_path = os.path.join(output_dir, "migration-report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        engine = "AST" if HAS_AST else "regex"
        f.write(f"é·ç§»å ±å‘Š (Migration Report) â€” v4 ({engine} engine)\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"ç¸½è¦å‰‡æ•¸: {len(results)}\n")
        f.write(f"  âœ… å®Œç¾è§£æ: {len(perfect)}\n")
        f.write(f"  âš ï¸  è¤‡é›œè¡¨é”å¼ (å·²è‡ªå‹•çŒœæ¸¬): {len(complex_rules)}\n")
        f.write(f"  ğŸš¨ ç„¡æ³•è§£æ (éœ€ LLM å”åŠ©): {len(unparseable)}\n")
        f.write(f"  ğŸ“– å»ºè­°ä½¿ç”¨é»ƒé‡‘æ¨™æº–: {len(golden_matches)}\n\n")

        # æ”¶æ–‚ç‡çµ±è¨ˆ â€” æ’é™¤ unparseable çš„ golden matches é¿å…å¤šæ‰£
        golden_parseable = len([r for r in results
                                if r.triage_action == "use_golden"
                                and r.status != "unparseable"])
        convertible = len(perfect) + len(complex_rules) - golden_parseable
        if convertible > 0:
            f.write(f"ğŸ“Š æ”¶æ–‚ç‡çµ±è¨ˆ:\n")
            f.write(f"  è¼¸å…¥: {len(results)} æ¢å‚³çµ±è¦å‰‡\n")
            f.write(f"  è¼¸å‡º: {total_output} æ¢ Recording Rules "
                    f"+ {convertible} æ¢ Alert Rules\n")
            if total_input > 0:
                compression = round((1 - total_output / max(total_input * 2, 1)) * 100, 1)
                f.write(f"  å£“ç¸®ç‡: {compression}%\n")
            f.write("\n")

        if golden_matches:
            f.write("-" * 40 + "\n")
            f.write("ğŸ“– å»ºè­°ä½¿ç”¨é»ƒé‡‘æ¨™æº– â€” è«‹ç”¨ scaffold_tenant.py è¨­å®šé–¾å€¼\n")
            f.write("-" * 40 + "\n")
            for r in golden_matches:
                golden = r.dict_match
                f.write(f"  â€¢ {r.alert_name}\n")
                f.write(f"    â†’ é»ƒé‡‘æ¨™æº–: {golden.get('golden_rule', '?')}\n")
                f.write(f"    â†’ Metric Key: {golden.get('maps_to', '?')}\n")
                f.write(f"    â†’ Rule Pack: {golden.get('rule_pack', '?')}\n")
                f.write(f"    â†’ {golden.get('note', '')}\n")
            f.write("\n")

        if perfect:
            f.write("-" * 40 + "\n")
            f.write("âœ… å®Œç¾è§£æçš„è¦å‰‡\n")
            f.write("-" * 40 + "\n")
            for r in perfect:
                if r.triage_action == "use_golden":
                    continue
                f.write(f"  â€¢ {r.alert_name}: {r.agg_mode} ({r.agg_reason})\n")
            f.write("\n")

        if complex_rules:
            f.write("-" * 40 + "\n")
            f.write("âš ï¸  è¤‡é›œè¡¨é”å¼ â€” å·²è‡ªå‹•çŒœæ¸¬èšåˆæ¨¡å¼ï¼Œå»ºè­°äººå·¥ç¢ºèª\n")
            f.write("-" * 40 + "\n")
            for r in complex_rules:
                if r.triage_action == "use_golden":
                    continue
                f.write(f"  â€¢ {r.alert_name}: {r.agg_mode} ({r.agg_reason})\n")
                if r.dim_hints:
                    f.write(f"    ğŸ“ ç¶­åº¦æ¨™ç±¤åµæ¸¬: {r.dim_hints}\n")
            f.write("\n")

        if unparseable:
            f.write("-" * 40 + "\n")
            f.write("ğŸš¨ ç„¡æ³•è‡ªå‹•è§£æ â€” è«‹å°‡ä»¥ä¸‹ LLM Prompt äº¤çµ¦ Claude è™•ç†\n")
            f.write("-" * 40 + "\n")
            for r in unparseable:
                if r.triage_action == "use_golden":
                    continue
                f.write(f"\n### {r.alert_name} ###\n")
                f.write(r.llm_prompt)
                f.write("\n")
    os.chmod(report_path, 0o600)

    # --- v3: Triage CSV ---
    csv_path = write_triage_csv(results, output_dir, dictionary)

    # --- v3: Prefix Mapping ---
    mapping_path = write_prefix_mapping(results, output_dir, prefix)

    return len(perfect), len(complex_rules), len(unparseable), golden_parseable


def print_dry_run(results):
    """Dry-run æ¨¡å¼: åƒ…åœ¨ STDOUT è¼¸å‡ºå ±å‘Šæ‘˜è¦ã€‚"""
    perfect = [r for r in results if r.status == "perfect"]
    complex_rules = [r for r in results if r.status == "complex"]
    unparseable = [r for r in results if r.status == "unparseable"]
    golden_matches = [r for r in results if r.triage_action == "use_golden"]

    print(f"\n{'='*60}")
    print("ğŸ” Dry-Run é è¦½ (ä¸ç”¢ç”Ÿæª”æ¡ˆ)")
    print(f"{'='*60}\n")
    print(f"ç¸½è¦å‰‡æ•¸: {len(results)}")
    print(f"  âœ… å®Œç¾è§£æ: {len(perfect)}")
    print(f"  âš ï¸  è¤‡é›œè¡¨é”å¼ (è‡ªå‹•çŒœæ¸¬): {len(complex_rules)}")
    print(f"  ğŸš¨ ç„¡æ³•è§£æ (éœ€ LLM): {len(unparseable)}")
    print(f"  ğŸ“– å»ºè­°ä½¿ç”¨é»ƒé‡‘æ¨™æº–: {len(golden_matches)}\n")

    for r in results:
        if r.triage_action == "use_golden":
            golden = r.dict_match
            print(f"  ğŸ“– {r.alert_name}: å»ºè­°æ”¹ç”¨é»ƒé‡‘æ¨™æº– "
                  f"{golden.get('golden_rule', '?')} (scaffold_tenant.py)")
        elif r.status == "unparseable":
            print(f"  ğŸš¨ {r.alert_name}: ç„¡æ³•è‡ªå‹•è§£æ (éœ€ LLM å”åŠ©)")
        else:
            icon = "âœ…" if r.status == "perfect" else "âš ï¸"
            print(f"  {icon} {r.alert_name}: {r.agg_mode} â€” {r.agg_reason}")
            for k, v in r.tenant_config.items():
                print(f"     â†’ {k}: \"{v}\"")
            if r.dim_hints:
                print(f"     ğŸ“ ç¶­åº¦: {r.dim_hints}")
    print()


def print_triage(results):
    """Triage æ¨¡å¼: ç²¾ç°¡çµ±è¨ˆ + CSV è·¯å¾‘æŒ‡å¼•ã€‚"""
    auto = [r for r in results if r.triage_action == "auto"]
    review = [r for r in results if r.triage_action == "review"]
    skip = [r for r in results if r.triage_action == "skip"]
    golden = [r for r in results if r.triage_action == "use_golden"]

    print(f"\n{'='*60}")
    print("ğŸ“Š Triage åˆ†æå ±å‘Š (å¤§è¦æ¨¡é·ç§»å‰ç½®åˆ†æ)")
    print(f"{'='*60}\n")
    print(f"ç¸½è¦å‰‡æ•¸: {len(results)}\n")
    print(f"  âœ… å¯è‡ªå‹•è½‰æ› (auto):      {len(auto):>4} æ¢")
    print(f"  âš ï¸  éœ€äººå·¥ç¢ºèª (review):     {len(review):>4} æ¢")
    print(f"  ğŸš¨ ç„¡æ³•è½‰æ› (skip):         {len(skip):>4} æ¢")
    print(f"  ğŸ“– å»ºè­°é»ƒé‡‘æ¨™æº– (use_golden): {len(golden):>4} æ¢\n")

    # æ”¶æ–‚ç‡
    unique_records = set()
    for r in results:
        if r.status != "unparseable":
            for rr in r.recording_rules:
                unique_records.add(rr["record"])
    convertible = len(auto) + len(review)
    if convertible > 0:
        print(f"ğŸ“ˆ é ä¼°æ”¶æ–‚ç‡:")
        print(f"   {convertible} æ¢å¯è½‰æ›è¦å‰‡ â†’ {len(unique_records)} æ¢ Recording Rules")
        compression = round((1 - len(unique_records) / max(convertible * 2, 1)) * 100, 1)
        print(f"   å£“ç¸®ç‡: {compression}%\n")

    if golden:
        print(f"ğŸ’¡ å»ºè­°:")
        print(f"   {len(golden)} æ¢è¦å‰‡å·²æœ‰é»ƒé‡‘æ¨™æº–è¦†è“‹ï¼Œå»ºè­°ç›´æ¥ç”¨")
        print(f"   scaffold_tenant.py è¨­å®šé–¾å€¼ï¼Œä¸éœ€è¦è½‰æ›ã€‚\n")


# ============================================================
# Main
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="å‚³çµ± Prometheus è­¦å ±è¦å‰‡é·ç§»è¼”åŠ©å·¥å…· (v4 AST) â€” è‡ªå‹•è½‰æ›ç‚ºå‹•æ…‹å¤šç§Ÿæˆ¶ä¸‰ä»¶å¥—"
    )
    parser.add_argument("input_file", help="å‚³çµ± Prometheus alert rules YAML æª”æ¡ˆ")
    parser.add_argument("-o", "--output-dir", default="migration_output",
                        help="è¼¸å‡ºç›®éŒ„ (é è¨­: migration_output)")
    parser.add_argument("--dry-run", action="store_true",
                        help="åƒ…é¡¯ç¤ºå ±å‘Šï¼Œä¸ç”¢ç”Ÿæª”æ¡ˆ")
    parser.add_argument("--interactive", action="store_true",
                        help="é‡åˆ°è¤‡é›œè¡¨é”å¼æ™‚äº’å‹•è©¢å•èšåˆæ¨¡å¼")
    parser.add_argument("--triage", action="store_true",
                        help="Triage æ¨¡å¼: åªç”¢å‡º CSV åˆ†æ¡¶å ±å‘Š (é©åˆå¤§è¦æ¨¡é·ç§»å‰ç½®åˆ†æ)")
    parser.add_argument("--no-prefix", action="store_true",
                        help="åœç”¨ custom_ å‰ç¶´éš”é›¢ (ä¸å»ºè­°: å¯èƒ½èˆ‡é»ƒé‡‘æ¨™æº–è¡çª)")
    parser.add_argument("--prefix", default="custom_",
                        help="è‡ªè¨‚å‰ç¶´ (é è¨­: custom_)")
    parser.add_argument("--no-dictionary", action="store_true",
                        help="åœç”¨å•Ÿç™¼å¼å­—å…¸æ¯”å°")
    parser.add_argument("--no-ast", action="store_true",
                        help="åœç”¨ AST å¼•æ“ï¼Œå¼·åˆ¶ä½¿ç”¨èˆŠç‰ˆ regex è§£æ (é™¤éŒ¯ç”¨)")
    args = parser.parse_args()

    # ç¢ºå®š prefix
    prefix = "" if args.no_prefix else args.prefix

    # AST å¼•æ“
    use_ast = (not args.no_ast) and HAS_AST
    if not args.no_ast and not HAS_AST:
        print("[WARN] promql-parser æœªå®‰è£ï¼Œé™ç´šç‚º regex å¼•æ“ã€‚"
              "å®‰è£: pip install promql-parser", file=sys.stderr)

    # è¼‰å…¥å­—å…¸
    dictionary = {} if args.no_dictionary else load_metric_dictionary()

    try:
        with open(args.input_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
    except Exception as e:
        print(f"Error reading YAML file: {e}", file=sys.stderr)
        sys.exit(1)

    groups = data.get('groups', [])
    if not groups:
        print("No 'groups' found in YAML.")
        return

    # è™•ç†æ‰€æœ‰è¦å‰‡
    results = []
    for group in groups:
        rules = group.get('rules', [])
        for rule in rules:
            result = process_rule(
                rule,
                interactive=args.interactive,
                prefix=prefix,
                dictionary=dictionary,
                use_ast=use_ast,
            )
            if result:
                results.append(result)

    if not results:
        print("No alert rules found to process.")
        return

    # Auto-Suppression: warning â†” critical é…å°
    n_paired = apply_auto_suppression(results)
    if n_paired:
        print(f"[ğŸ”—] Auto-Suppression: {n_paired} çµ„ warningâ†”critical é…å°å®Œæˆ")

    # è¼¸å‡º
    if args.triage:
        # Triage mode: åªç”¢ CSV + çµ±è¨ˆ
        os.makedirs(args.output_dir, exist_ok=True)
        csv_path = write_triage_csv(results, args.output_dir, dictionary)
        print_triage(results)
        print(f"ğŸ“ CSV å ±å‘Šå·²è¼¸å‡ºè‡³ {csv_path}")
        print(f"   è«‹åœ¨ Excel/Google Sheets ä¸­é–‹å•Ÿï¼Œæ‰¹æ¬¡æ±ºç­–æ¯æ¢è¦å‰‡çš„è™•ç†æ–¹å¼ã€‚\n")
    elif args.dry_run:
        print_dry_run(results)
    else:
        n_perfect, n_complex, n_unparseable, n_golden = write_outputs(
            results, args.output_dir, prefix, dictionary
        )
        convertible = n_perfect + n_complex - n_golden
        print(f"[âœ“] æˆåŠŸè½‰æ› {convertible} æ¢è¦å‰‡ "
              f"(âœ… {n_perfect} å®Œç¾, âš ï¸ {n_complex} å·²çŒœæ¸¬)")
        if n_golden:
            print(f"[ğŸ“–] {n_golden} æ¢å»ºè­°æ”¹ç”¨é»ƒé‡‘æ¨™æº– (è©³è¦‹å ±å‘Š)")
        if n_unparseable:
            print(f"[!] {n_unparseable} æ¢éœ€äººå·¥è™•ç† (LLM Prompt å·²å¯«å…¥å ±å‘Š)")
        print(f"ğŸ“ æª”æ¡ˆå·²è¼¸å‡ºè‡³ {args.output_dir}/")
        if prefix:
            print(f"ğŸ·ï¸  å‰ç¶´: {prefix} (Prefix Mapping å·²è¼¸å‡º)")


if __name__ == "__main__":
    main()
