#!/usr/bin/env python3
"""validate_migration.py â€” Shadow Monitoring é©—è­‰å·¥å…·ã€‚

æ¯”å°æ–°èˆŠ Recording Rule çš„æ•¸å€¼è¼¸å‡ºï¼Œé©—è­‰é·ç§»å¾Œçš„è¡Œç‚ºç­‰åƒ¹æ€§ã€‚
é€é Prometheus API æŠ“å–å…©çµ„ Recording Rule çš„å³æ™‚å‘é‡å€¼ï¼Œ
é€ä¸€æ¯”å°æ•¸å€¼å·®ç•°ï¼Œç”¢å‡º diff å ±å‘Šã€‚

ç”¨æ³•:
  # åŸºæœ¬æ¯”å°: æŒ‡å®šæ–°èˆŠå…©çµ„ recording rule åç¨±
  python3 validate_migration.py \\
    --old "mysql_global_status_threads_connected" \\
    --new "tenant:custom_mysql_global_status_threads_connected:max" \\
    --prometheus http://prometheus.monitoring.svc.cluster.local:9090

  # æ‰¹æ¬¡æ¯”å°: ä½¿ç”¨ prefix-mapping.yaml
  python3 validate_migration.py \\
    --mapping migration_output/prefix-mapping.yaml \\
    --prometheus http://prometheus.monitoring.svc.cluster.local:9090

  # æŒçºŒç›£æ§æ¨¡å¼ (æ¯ 60 ç§’æ¯”å°ä¸€æ¬¡ï¼Œé‹è¡Œ N è¼ª)
  python3 validate_migration.py \\
    --mapping migration_output/prefix-mapping.yaml \\
    --prometheus http://prometheus.monitoring.svc.cluster.local:9090 \\
    --watch --interval 60 --rounds 1440

  # æœ¬åœ°é–‹ç™¼ (é€é port-forward)
  kubectl port-forward svc/prometheus 9090:9090 -n monitoring &
  python3 validate_migration.py \\
    --mapping migration_output/prefix-mapping.yaml \\
    --prometheus http://localhost:9090

  # å¢é›†å…§åŸ·è¡Œ (åŒ…è£ç‚º K8s Jobï¼Œé©åˆé•·æœŸ Shadow Monitoring)
  # åƒè¦‹ docs/migration-guide.md Â§11 çš„ Job manifest ç¯„ä¾‹

éœ€æ±‚:
  - Prometheus Query API å¿…é ˆå¯å¾è…³æœ¬åŸ·è¡Œä½ç½®å­˜å–
    * å¢é›†å…§: K8s Service (http://prometheus.monitoring.svc.cluster.local:9090)
    * å¢é›†å¤–: port-forward æˆ– Ingress
    * å¤šå¢é›†: Thanos Query / VictoriaMetrics ç­‰çµ±ä¸€æŸ¥è©¢ç«¯é»äº¦å¯
  - æ–°èˆŠå…©å¥— Recording Rule å¿…é ˆåŒæ™‚åœ¨ Prometheus ä¸­é‹è¡Œ
  - å»ºè­°åœ¨ Shadow Monitoring éšæ®µ (æ–° Alert æ› migration_status=shadow) ä½¿ç”¨
"""

import sys
import os
import csv
import json
import time
import argparse
import urllib.request
import urllib.parse
import yaml


def query_prometheus(prom_url, promql):
    """åŸ·è¡Œ Prometheus instant queryï¼Œå›å‚³çµæœå‘é‡ã€‚"""
    url = f"{prom_url}/api/v1/query"
    params = urllib.parse.urlencode({"query": promql})
    full_url = f"{url}?{params}"

    try:
        req = urllib.request.Request(full_url)  # nosec B310 â€” localhost only
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read().decode())
    except Exception as e:
        return None, str(e)

    if data.get("status") != "success":
        return None, data.get("error", "Unknown error")

    results = data.get("data", {}).get("result", [])
    return results, None


def extract_value_map(results, group_by="tenant"):
    """å°‡ Prometheus çµæœè½‰æ›ç‚º {label_key: float_value} å­—å…¸ã€‚"""
    value_map = {}
    for item in results:
        metric = item.get("metric", {})
        key = metric.get(group_by, "__no_label__")
        val_str = item.get("value", [None, None])[1]
        try:
            value_map[key] = float(val_str)
        except (TypeError, ValueError):
            value_map[key] = None
    return value_map


def compare_vectors(old_map, new_map, tolerance=0.001):
    """æ¯”å°å…©çµ„å‘é‡å€¼ï¼Œå›å‚³å·®ç•°æ¸…å–®ã€‚

    tolerance: å…è¨±çš„æ•¸å€¼èª¤å·® (é è¨­ 0.1%)
    """
    diffs = []
    all_keys = sorted(set(list(old_map.keys()) + list(new_map.keys())))

    for key in all_keys:
        old_val = old_map.get(key)
        new_val = new_map.get(key)

        if old_val is None and new_val is None:
            status = "both_empty"
        elif old_val is None:
            status = "old_missing"
        elif new_val is None:
            status = "new_missing"
        elif abs(old_val - new_val) <= tolerance * max(abs(old_val), abs(new_val), 1):
            status = "match"
        else:
            status = "mismatch"

        diffs.append({
            "tenant": key,
            "old_value": old_val,
            "new_value": new_val,
            "status": status,
            "delta": (new_val - old_val) if old_val is not None and new_val is not None else None,
        })
    return diffs


def run_single_comparison(prom_url, old_query, new_query, label):
    """åŸ·è¡Œå–®æ¬¡æ¯”å°ï¼Œå›å‚³ diff çµæœã€‚"""
    old_results, old_err = query_prometheus(prom_url, old_query)
    if old_err:
        print(f"  âŒ æŸ¥è©¢èˆŠè¦å‰‡å¤±æ•—: {old_err}", file=sys.stderr)
        return None

    new_results, new_err = query_prometheus(prom_url, new_query)
    if new_err:
        print(f"  âŒ æŸ¥è©¢æ–°è¦å‰‡å¤±æ•—: {new_err}", file=sys.stderr)
        return None

    old_map = extract_value_map(old_results, group_by="tenant")
    new_map = extract_value_map(new_results, group_by="tenant")

    diffs = compare_vectors(old_map, new_map)
    return {
        "label": label,
        "old_query": old_query,
        "new_query": new_query,
        "old_count": len(old_results),
        "new_count": len(new_results),
        "diffs": diffs,
    }


def load_mapping_pairs(mapping_path):
    """å¾ prefix-mapping.yaml è¼‰å…¥æ¯”å°çµ„ã€‚"""
    with open(mapping_path, 'r', encoding='utf-8') as f:
        mapping = yaml.safe_load(f) or {}

    pairs = []
    for prefixed_key, info in mapping.items():
        original = info.get("original_metric")
        if not original:
            continue
        # æ§‹é€ æ–°èˆŠ Recording Rule åç¨±
        # èˆŠ: åŸå§‹ metric name (ç›´æ¥æŸ¥è©¢)
        # æ–°: tenant:<prefixed_key>:<agg> (éœ€çŒœæ¸¬ aggï¼Œä½† recording rule å·²å­˜åœ¨)
        pairs.append({
            "label": prefixed_key,
            "old_query": original,
            "new_query": f"tenant:{prefixed_key}:max",  # é è¨­ maxï¼Œä½¿ç”¨è€…å¯åœ¨ CSV ä¸­ä¿®æ”¹
            "alert_name": info.get("alert_name", ""),
            "golden_match": info.get("golden_match"),
        })
    return pairs


def print_summary(all_results):
    """å°å‡ºæ¯”å°æ‘˜è¦ã€‚"""
    total_pairs = len(all_results)
    total_matches = 0
    total_mismatches = 0
    total_missing = 0

    for result in all_results:
        if result is None:
            continue
        for d in result["diffs"]:
            if d["status"] == "match":
                total_matches += 1
            elif d["status"] == "mismatch":
                total_mismatches += 1
            elif d["status"] in ("old_missing", "new_missing"):
                total_missing += 1

    print(f"\n{'='*60}")
    print("ğŸ“Š é©—è­‰æ‘˜è¦ (Validation Summary)")
    print(f"{'='*60}\n")
    print(f"æ¯”å°çµ„æ•¸: {total_pairs}")
    print(f"  âœ… æ•¸å€¼ä¸€è‡´: {total_matches}")
    print(f"  âŒ æ•¸å€¼å·®ç•°: {total_mismatches}")
    print(f"  âš ï¸  ç¼ºå°‘è³‡æ–™: {total_missing}\n")

    if total_mismatches == 0 and total_missing == 0:
        print("ğŸ‰ æ‰€æœ‰ Recording Rule æ•¸å€¼å®Œå…¨ä¸€è‡´ï¼å¯ä»¥å®‰å…¨åˆ‡æ›ã€‚\n")
    elif total_mismatches > 0:
        print("âš ï¸  ç™¼ç¾æ•¸å€¼å·®ç•°ï¼Œè«‹æª¢æŸ¥ä»¥ä¸‹é …ç›®:\n")
        for result in all_results:
            if result is None:
                continue
            for d in result["diffs"]:
                if d["status"] == "mismatch":
                    print(f"  â€¢ [{result['label']}] tenant={d['tenant']}: "
                          f"èˆŠ={d['old_value']} æ–°={d['new_value']} "
                          f"(å·®ç•°={d['delta']:.4f})")
        print()


def write_csv_report(all_results, output_dir):
    """å°‡æ¯”å°çµæœå¯«å…¥ CSVã€‚"""
    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, "validation-report.csv")

    with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow([
            "Label", "Tenant", "Old Query", "New Query",
            "Old Value", "New Value", "Delta", "Status",
            "Timestamp",
        ])
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        for result in all_results:
            if result is None:
                continue
            for d in result["diffs"]:
                writer.writerow([
                    result["label"],
                    d["tenant"],
                    result["old_query"],
                    result["new_query"],
                    d["old_value"],
                    d["new_value"],
                    d["delta"],
                    d["status"],
                    ts,
                ])
    os.chmod(csv_path, 0o600)
    return csv_path


def main():
    parser = argparse.ArgumentParser(
        description="Shadow Monitoring é©—è­‰å·¥å…· â€” æ¯”å°æ–°èˆŠ Recording Rule æ•¸å€¼",
    )

    # æ¯”å°ä¾†æº
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--mapping", help="prefix-mapping.yaml æª”æ¡ˆè·¯å¾‘ (æ‰¹æ¬¡æ¯”å°)")
    group.add_argument("--old", help="èˆŠ Recording Rule çš„ PromQL æŸ¥è©¢")

    parser.add_argument("--new", help="æ–° Recording Rule çš„ PromQL æŸ¥è©¢ (æ­é… --old)")
    parser.add_argument("--prometheus", default="http://localhost:9090",
                        help="Prometheus Query API URL "
                             "(é è¨­: http://localhost:9090; "
                             "å¢é›†å…§å»ºè­°ç”¨ http://prometheus.monitoring.svc.cluster.local:9090)")
    parser.add_argument("-o", "--output-dir", default="validation_output",
                        help="è¼¸å‡ºç›®éŒ„ (é è¨­: validation_output)")
    parser.add_argument("--tolerance", type=float, default=0.001,
                        help="æ•¸å€¼èª¤å·®å®¹å¿åº¦ (é è¨­: 0.001 = 0.1%%)")
    parser.add_argument("--watch", action="store_true",
                        help="æŒçºŒç›£æ§æ¨¡å¼")
    parser.add_argument("--interval", type=int, default=60,
                        help="ç›£æ§é–“éš”ç§’æ•¸ (é è¨­: 60)")
    parser.add_argument("--rounds", type=int, default=10,
                        help="ç›£æ§è¼ªæ•¸ (é è¨­: 10)")

    args = parser.parse_args()

    # æ§‹é€ æ¯”å°çµ„
    pairs = []
    if args.mapping:
        pairs = load_mapping_pairs(args.mapping)
        print(f"ğŸ“‚ è¼‰å…¥ {len(pairs)} çµ„æ¯”å°è‡ª {args.mapping}")
    elif args.old and args.new:
        pairs = [{
            "label": "manual",
            "old_query": args.old,
            "new_query": args.new,
        }]
    else:
        print("éŒ¯èª¤: ä½¿ç”¨ --old æ™‚å¿…é ˆåŒæ™‚æŒ‡å®š --new", file=sys.stderr)
        sys.exit(1)

    if not pairs:
        print("No comparison pairs found.")
        return

    def run_once():
        all_results = []
        for pair in pairs:
            print(f"  ğŸ” æ¯”å°: {pair['label']}...")
            result = run_single_comparison(
                args.prometheus,
                pair["old_query"],
                pair["new_query"],
                pair["label"],
            )
            all_results.append(result)
        return all_results

    if args.watch:
        print(f"\nğŸ‘ï¸  æŒçºŒç›£æ§æ¨¡å¼: æ¯ {args.interval} ç§’æ¯”å°ä¸€æ¬¡ï¼Œå…± {args.rounds} è¼ª\n")
        for i in range(args.rounds):
            print(f"\n--- ç¬¬ {i+1}/{args.rounds} è¼ª ({time.strftime('%H:%M:%S')}) ---")
            all_results = run_once()
            print_summary(all_results)
            csv_path = write_csv_report(all_results, args.output_dir)
            if i < args.rounds - 1:
                time.sleep(args.interval)
        print(f"\nğŸ“ æœ€çµ‚å ±å‘Š: {csv_path}")
    else:
        all_results = run_once()
        print_summary(all_results)
        csv_path = write_csv_report(all_results, args.output_dir)
        print(f"ğŸ“ CSV å ±å‘Š: {csv_path}")


if __name__ == "__main__":
    main()
