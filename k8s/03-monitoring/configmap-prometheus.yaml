apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    alerting:
      alertmanagers:
        - static_configs:
            - targets:
                - "alertmanager.monitoring.svc.cluster.local:9093"

    rule_files:
      - "/etc/prometheus/rules/*.yml"

    scrape_configs:
      # -------------------------------------------------------
      # Self-monitoring (static)
      # -------------------------------------------------------
      - job_name: "prometheus"
        static_configs:
          - targets: ["localhost:9090"]

      # -------------------------------------------------------
      # Annotation-based Service Discovery
      # Any Service with prometheus.io/scrape: "true" is auto-discovered.
      # New tenants or components no longer require Prometheus config changes.
      # -------------------------------------------------------

      # Tenant DB exporters (db-a, db-b, db-c, ...)
      # Discovers services in db-* namespaces.
      # 'tenant' label is derived from the namespace.
      - job_name: "tenant-exporters"
        scrape_interval: 10s
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          # Only keep services with prometheus.io/scrape annotation
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: "true"
          # Only keep services in db-* namespaces
          - source_labels: [__meta_kubernetes_namespace]
            action: keep
            regex: "db-.+"
          # Use the annotated port
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          # Use annotated metrics path
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          # Set tenant label from namespace
          - source_labels: [__meta_kubernetes_namespace]
            target_label: tenant
          # Set instance label from namespace (backward compat)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: instance
          # Add env label
          - target_label: env
            replacement: "test"

      # Monitoring namespace services
      # Auto-discovers: kube-state-metrics, threshold-exporter,
      # and any future component with the annotation.
      - job_name: "monitoring-components"
        scrape_interval: 15s
        kubernetes_sd_configs:
          - role: service
            namespaces:
              names: ["monitoring"]
        relabel_configs:
          # Only keep annotated services
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: "true"
          # Use the annotated port
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          # Use annotated metrics path
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          # Set job label from service name for clarity
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job

      # -------------------------------------------------------
      # Scenario B: kubelet cAdvisor metrics (container-level CPU/memory)
      # Uses K8s API proxy to avoid direct kubelet TLS issues in Kind.
      # Provides: container_cpu_usage_seconds_total, container_memory_working_set_bytes
      # -------------------------------------------------------
      - job_name: "kubelet-cadvisor"
        scrape_interval: 15s
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
        metric_relabel_configs:
          # Only keep container metrics from tenant namespaces
          - source_labels: [namespace]
            action: keep
            regex: "db-.+"
          # Drop high-cardinality metrics we don't need
          - source_labels: [__name__]
            action: keep
            regex: "container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_memory_usage_bytes"

  # ============================================================
  # Recording Rules (Normalization Layer)
  # All 5 rule packs are pre-loaded (no-metric → no-alert, near-zero cost).
  # Canonical sources: rule-packs/rule-pack-*.yaml
  # ============================================================
  recording-rules.yml: |
    groups:
      # -------------------------------------------------------
      # MariaDB normalization (from rule-pack-mariadb.yaml)
      # -------------------------------------------------------
      - name: mariadb-normalization
        interval: 15s
        rules:
          - record: tenant:mysql_cpu_usage:rate5m
            expr: sum by(tenant) (rate(mysql_global_status_threads_running[5m]))

          - record: tenant:mysql_threads_connected:sum
            expr: sum by(tenant) (mysql_global_status_threads_connected)

          - record: tenant:mysql_connection_usage:ratio
            expr: |
              sum by(tenant) (mysql_global_status_threads_connected)
              / sum by(tenant) (mysql_global_variables_max_connections)

          - record: tenant:mysql_uptime:hours
            expr: min by(tenant) (mysql_global_status_uptime) / 3600

          # Percona-inspired additional normalization
          - record: tenant:mysql_slow_queries:rate5m
            expr: sum by(tenant) (rate(mysql_global_status_slow_queries[5m]))

          - record: tenant:mysql_aborted_connections:rate5m
            expr: sum by(tenant) (rate(mysql_global_status_aborted_connects[5m]))

          - record: tenant:mysql_buffer_pool_usage:ratio
            expr: |
              sum by(tenant) (mysql_global_status_innodb_buffer_pool_pages_data)
              / sum by(tenant) (mysql_global_status_innodb_buffer_pool_pages_total)

      # -------------------------------------------------------
      # MariaDB threshold normalization (from rule-pack-mariadb.yaml)
      # -------------------------------------------------------
      - name: mariadb-threshold-normalization
        interval: 15s
        rules:
          - record: tenant:alert_threshold:connections
            expr: sum by(tenant) (user_threshold{metric="connections", severity="warning"})

          - record: tenant:alert_threshold:cpu
            expr: sum by(tenant) (user_threshold{metric="cpu", severity="warning"})

          # Multi-tier severity (Scenario D)
          - record: tenant:alert_threshold:connections_critical
            expr: sum by(tenant) (user_threshold{metric="connections", severity="critical"})

          - record: tenant:alert_threshold:cpu_critical
            expr: sum by(tenant) (user_threshold{metric="cpu", severity="critical"})

      # -------------------------------------------------------
      # Kubernetes container normalization (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-container-normalization
        interval: 15s
        rules:
          # Container CPU usage as % of limit
          - record: tenant:container_cpu_percent:by_container
            expr: |
              label_replace(
                sum by(namespace, pod, container) (
                  rate(container_cpu_usage_seconds_total{namespace=~"db-.+", container!="", container!="POD"}[5m])
                )
                /
                sum by(namespace, pod, container) (
                  kube_pod_container_resource_limits{resource="cpu", namespace=~"db-.+"}
                )
                * 100,
                "tenant", "$1", "namespace", "(.*)"
              )

          # Container memory usage as % of limit
          - record: tenant:container_memory_percent:by_container
            expr: |
              label_replace(
                sum by(namespace, pod, container) (
                  container_memory_working_set_bytes{namespace=~"db-.+", container!="", container!="POD"}
                )
                /
                sum by(namespace, pod, container) (
                  kube_pod_container_resource_limits{resource="memory", namespace=~"db-.+"}
                )
                * 100,
                "tenant", "$1", "namespace", "(.*)"
              )

          # Weakest link: MAX across containers per pod
          - record: tenant:pod_weakest_cpu_percent:max
            expr: max by(tenant, pod) (tenant:container_cpu_percent:by_container)

          - record: tenant:pod_weakest_memory_percent:max
            expr: max by(tenant, pod) (tenant:container_memory_percent:by_container)

      # -------------------------------------------------------
      # Kubernetes state matching (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-state-matching
        interval: 15s
        rules:
          - record: tenant:container_waiting_reason:count
            expr: |
              label_replace(
                count by(namespace, reason) (
                  kube_pod_container_status_waiting_reason{namespace=~"db-.+"} > 0
                ),
                "tenant", "$1", "namespace", "(.*)"
              )

      # -------------------------------------------------------
      # Kubernetes threshold normalization (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-threshold-normalization
        interval: 15s
        rules:
          - record: tenant:alert_threshold:container_cpu
            expr: sum by(tenant) (user_threshold{component="container", metric="cpu"})

          - record: tenant:alert_threshold:container_memory
            expr: sum by(tenant) (user_threshold{component="container", metric="memory"})

      # -------------------------------------------------------
      # Redis normalization (from rule-pack-redis.yaml)
      # -------------------------------------------------------
      - name: redis-normalization
        interval: 15s
        rules:
          - record: tenant:redis_memory_used_bytes:max
            expr: max by(tenant) (redis_memory_used_bytes)

          - record: tenant:redis_memory_usage:ratio
            expr: |
              max by(tenant) (redis_memory_used_bytes)
              / max by(tenant) (redis_memory_max_bytes > 0)

          - record: tenant:redis_connected_clients:max
            expr: max by(tenant) (redis_connected_clients)

          - record: tenant:redis_evicted_keys:rate5m
            expr: sum by(tenant) (rate(redis_evicted_keys_total[5m]))

          - record: tenant:redis_keyspace_hit_ratio:avg
            expr: |
              sum by(tenant) (rate(redis_keyspace_hits_total[5m]))
              / (
                sum by(tenant) (rate(redis_keyspace_hits_total[5m]))
                + sum by(tenant) (rate(redis_keyspace_misses_total[5m]))
              )

          - record: tenant:redis_replication_lag:max
            expr: max by(tenant) (redis_connected_slave_lag_seconds)

          - record: tenant:redis_commands:rate5m
            expr: sum by(tenant) (rate(redis_commands_processed_total[5m]))

      # -------------------------------------------------------
      # Redis threshold normalization (from rule-pack-redis.yaml)
      # -------------------------------------------------------
      - name: redis-threshold-normalization
        interval: 15s
        rules:
          - record: tenant:alert_threshold:redis_memory_used_bytes
            expr: sum by(tenant) (user_threshold{metric="redis_memory_used_bytes", severity="warning"})

          - record: tenant:alert_threshold:redis_connected_clients
            expr: sum by(tenant) (user_threshold{metric="redis_connected_clients", severity="warning"})

          - record: tenant:alert_threshold:redis_evicted_keys_rate
            expr: sum by(tenant) (user_threshold{metric="redis_evicted_keys_rate", severity="warning"})

          - record: tenant:alert_threshold:redis_replication_lag
            expr: sum by(tenant) (user_threshold{metric="redis_replication_lag", severity="warning"})

      # -------------------------------------------------------
      # MongoDB normalization (from rule-pack-mongodb.yaml)
      # -------------------------------------------------------
      - name: mongodb-normalization
        interval: 15s
        rules:
          - record: tenant:mongodb_connections_current:max
            expr: max by(tenant) (mongodb_connections{state="current"})

          - record: tenant:mongodb_connections_available:min
            expr: min by(tenant) (mongodb_connections{state="available"})

          - record: tenant:mongodb_connection_usage:ratio
            expr: |
              max by(tenant) (mongodb_connections{state="current"})
              / (
                max by(tenant) (mongodb_connections{state="current"})
                + min by(tenant) (mongodb_connections{state="available"})
              )

          - record: tenant:mongodb_replication_lag:max
            expr: max by(tenant) (mongodb_mongod_replset_member_replication_lag)

          - record: tenant:mongodb_opcounters:rate5m
            expr: sum by(tenant) (rate(mongodb_opcounters_total[5m]))

          - record: tenant:mongodb_document_ops:rate5m
            expr: sum by(tenant) (rate(mongodb_mongod_metrics_document_total[5m]))

          - record: tenant:mongodb_page_faults:rate5m
            expr: sum by(tenant) (rate(mongodb_extra_info_page_faults_total[5m]))

      # -------------------------------------------------------
      # MongoDB threshold normalization (from rule-pack-mongodb.yaml)
      # -------------------------------------------------------
      - name: mongodb-threshold-normalization
        interval: 15s
        rules:
          - record: tenant:alert_threshold:mongodb_connections_current
            expr: sum by(tenant) (user_threshold{metric="mongodb_connections_current", severity="warning"})

          - record: tenant:alert_threshold:mongodb_replication_lag
            expr: sum by(tenant) (user_threshold{metric="mongodb_replication_lag", severity="warning"})

          - record: tenant:alert_threshold:mongodb_opcounters_rate
            expr: sum by(tenant) (user_threshold{metric="mongodb_opcounters_rate", severity="warning"})

      # -------------------------------------------------------
      # Elasticsearch normalization (from rule-pack-elasticsearch.yaml)
      # -------------------------------------------------------
      - name: elasticsearch-normalization
        interval: 15s
        rules:
          - record: tenant:es_cluster_health:status
            expr: max by(tenant) (elasticsearch_cluster_health_status{color="red"}) * 2
                  + max by(tenant) (elasticsearch_cluster_health_status{color="yellow"})

          - record: tenant:es_heap_usage_percent:max
            expr: |
              max by(tenant) (
                elasticsearch_jvm_memory_used_bytes{area="heap"}
                / elasticsearch_jvm_memory_max_bytes{area="heap"}
              ) * 100

          - record: tenant:es_disk_usage_percent:max
            expr: |
              max by(tenant) (
                1 - (
                  elasticsearch_filesystem_data_available_bytes
                  / elasticsearch_filesystem_data_size_bytes
                )
              ) * 100

          - record: tenant:es_search_latency_ms:avg
            expr: |
              sum by(tenant) (rate(elasticsearch_indices_search_query_time_seconds[5m]))
              / sum by(tenant) (rate(elasticsearch_indices_search_query_total[5m]))
              * 1000

          - record: tenant:es_indexing:rate5m
            expr: sum by(tenant) (rate(elasticsearch_indices_indexing_index_total[5m]))

          - record: tenant:es_pending_tasks:max
            expr: max by(tenant) (elasticsearch_cluster_health_number_of_pending_tasks)

          - record: tenant:es_unassigned_shards:count
            expr: max by(tenant) (elasticsearch_cluster_health_unassigned_shards)

      # -------------------------------------------------------
      # Elasticsearch threshold normalization (from rule-pack-elasticsearch.yaml)
      # -------------------------------------------------------
      - name: elasticsearch-threshold-normalization
        interval: 15s
        rules:
          - record: tenant:alert_threshold:es_heap_usage_percent
            expr: sum by(tenant) (user_threshold{metric="es_heap_usage_percent", severity="warning"})

          - record: tenant:alert_threshold:es_disk_usage_percent
            expr: sum by(tenant) (user_threshold{metric="es_disk_usage_percent", severity="warning"})

          - record: tenant:alert_threshold:es_search_latency_ms
            expr: sum by(tenant) (user_threshold{metric="es_search_latency_ms", severity="warning"})

          - record: tenant:alert_threshold:es_pending_tasks
            expr: sum by(tenant) (user_threshold{metric="es_pending_tasks", severity="warning"})

  # ============================================================
  # Alert Rules (using normalized metrics)
  # All 5 rule packs are pre-loaded (no-metric → no-alert, near-zero cost).
  # Canonical sources: rule-packs/rule-pack-*.yaml
  # ============================================================
  alert-rules.yml: |
    groups:
      # -------------------------------------------------------
      # MariaDB alerts (from rule-pack-mariadb.yaml)
      # -------------------------------------------------------
      - name: mariadb-alerts
        rules:
          - alert: MariaDBDown
            expr: mysql_up == 0
            for: 15s
            labels:
              severity: critical
            annotations:
              summary: "MariaDB instance {{ $labels.instance }} is DOWN"
              description: "mysql_up=0 for 15s on {{ $labels.instance }}"

          - alert: MariaDBExporterAbsent
            expr: absent(mysql_up{job="tenant-exporters"})
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "mysqld-exporter target missing"
              description: "No mysql_up metric found for 30s"

          - alert: MariaDBHighConnections
            expr: |
              (
                tenant:mysql_threads_connected:sum
                > on(tenant) group_left
                tenant:alert_threshold:connections
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
              unless on(tenant)
              (
                tenant:mysql_threads_connected:sum
                > on(tenant) group_left
                tenant:alert_threshold:connections_critical
              )
            for: 30s
            labels:
              severity: warning
            annotations:
              summary: "High connections on {{ $labels.tenant }}"
              description: "{{ $value }} threads connected (warning threshold exceeded)"

          # Critical tier — suppresses warning via unless above
          - alert: MariaDBHighConnectionsCritical
            expr: |
              (
                tenant:mysql_threads_connected:sum
                > on(tenant) group_left
                tenant:alert_threshold:connections_critical
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "Critical connections on {{ $labels.tenant }}"
              description: "{{ $value }} threads connected (critical threshold exceeded)"

          # Composite alert: connections AND CPU both exceeded
          - alert: MariaDBSystemBottleneck
            expr: |
              (
                (
                  tenant:mysql_threads_connected:sum
                  > on(tenant) group_left
                  tenant:alert_threshold:connections
                )
                and on(tenant)
                (
                  tenant:mysql_cpu_usage:rate5m
                  > on(tenant) group_left
                  tenant:alert_threshold:cpu
                )
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 60s
            labels:
              severity: critical
            annotations:
              summary: "System bottleneck on {{ $labels.tenant }}"
              description: "Both connections AND CPU thresholds exceeded simultaneously"

          - alert: MariaDBRecentRestart
            expr: mysql_global_status_uptime < 300
            for: 0s
            labels:
              severity: info
            annotations:
              summary: "{{ $labels.instance }} restarted recently"
              description: "Uptime is only {{ $value }}s (< 5 min)"

          # Percona-inspired: High slow queries rate
          - alert: MariaDBHighSlowQueries
            expr: |
              tenant:mysql_slow_queries:rate5m > 1
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High slow query rate on {{ $labels.tenant }}"
              description: "{{ $value | printf \"%.2f\" }} slow queries/sec"

          # Percona-inspired: High aborted connections
          - alert: MariaDBHighAbortedConnections
            expr: |
              tenant:mysql_aborted_connections:rate5m > 5
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High aborted connections on {{ $labels.tenant }}"
              description: "{{ $value | printf \"%.2f\" }} aborted connections/sec"

      # -------------------------------------------------------
      # Kubernetes container alerts (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-container-alerts
        rules:
          - alert: PodContainerHighCPU
            expr: |
              (
                max by(tenant) (tenant:pod_weakest_cpu_percent:max)
                > on(tenant) group_left
                tenant:alert_threshold:container_cpu
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 60s
            labels:
              severity: warning
            annotations:
              summary: "High container CPU in {{ $labels.tenant }}"
              description: "Max container CPU usage: {{ $value | printf \"%.1f\" }}% (threshold exceeded)"

          - alert: PodContainerHighMemory
            expr: |
              (
                max by(tenant) (tenant:pod_weakest_memory_percent:max)
                > on(tenant) group_left
                tenant:alert_threshold:container_memory
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 60s
            labels:
              severity: warning
            annotations:
              summary: "High container memory in {{ $labels.tenant }}"
              description: "Max container memory usage: {{ $value | printf \"%.1f\" }}% (threshold exceeded)"

      # -------------------------------------------------------
      # Kubernetes pod state alerts (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-pod-state-alerts
        rules:
          - alert: ContainerCrashLoop
            expr: |
              (
                (
                  tenant:container_waiting_reason:count{reason="CrashLoopBackOff"}
                  * on(tenant) group_left
                  user_state_filter{filter="container_crashloop"}
                ) > 0
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "Container crash loop detected in {{ $labels.tenant }}"
              description: "{{ $value }} container(s) in CrashLoopBackOff state"

          - alert: ContainerImagePullFailure
            expr: |
              (
                (
                  tenant:container_waiting_reason:count{reason=~"ImagePullBackOff|InvalidImageName"}
                  * on(tenant) group_left
                  user_state_filter{filter="container_imagepull"}
                ) > 0
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 30s
            labels:
              severity: warning
            annotations:
              summary: "Image pull failure detected in {{ $labels.tenant }}"
              description: "{{ $value }} container(s) in ImagePullBackOff/InvalidImageName state"

      # -------------------------------------------------------
      # Redis alerts (from rule-pack-redis.yaml)
      # Pre-loaded: no-metric → no-alert (near-zero cost)
      # -------------------------------------------------------
      - name: redis-alerts
        rules:
          - alert: RedisDown
            expr: redis_up == 0
            for: 15s
            labels:
              severity: critical
            annotations:
              summary: "Redis instance {{ $labels.instance }} is DOWN"

          - alert: RedisHighMemory
            expr: |
              (
                tenant:redis_memory_used_bytes:max
                > on(tenant) group_left
                tenant:alert_threshold:redis_memory_used_bytes
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Redis high memory usage on {{ $labels.tenant }}"
              description: "Memory usage: {{ $value | humanize1024 }}B"

          - alert: RedisHighConnections
            expr: |
              (
                tenant:redis_connected_clients:max
                > on(tenant) group_left
                tenant:alert_threshold:redis_connected_clients
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Redis high connection count on {{ $labels.tenant }}"
              description: "{{ $value }} connected clients"

          - alert: RedisHighKeyEvictions
            expr: |
              (
                tenant:redis_evicted_keys:rate5m
                > on(tenant) group_left
                tenant:alert_threshold:redis_evicted_keys_rate
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Redis high key eviction rate on {{ $labels.tenant }}"
              description: "{{ $value | printf \"%.1f\" }} evictions/sec — consider increasing maxmemory"

          - alert: RedisReplicationLag
            expr: |
              (
                tenant:redis_replication_lag:max
                > on(tenant) group_left
                tenant:alert_threshold:redis_replication_lag
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "Redis replication lag on {{ $labels.tenant }}"
              description: "Replication lag: {{ $value | printf \"%.1f\" }}s"

          - alert: RedisLowHitRatio
            expr: |
              tenant:redis_keyspace_hit_ratio:avg < 0.9
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Redis low hit ratio on {{ $labels.tenant }}"
              description: "Hit ratio: {{ $value | printf \"%.2f\" }} — check cache strategy"

      # -------------------------------------------------------
      # MongoDB alerts (from rule-pack-mongodb.yaml)
      # Pre-loaded: no-metric → no-alert (near-zero cost)
      # -------------------------------------------------------
      - name: mongodb-alerts
        rules:
          - alert: MongoDBDown
            expr: mongodb_up == 0
            for: 15s
            labels:
              severity: critical
            annotations:
              summary: "MongoDB instance {{ $labels.instance }} is DOWN"

          - alert: MongoDBHighConnections
            expr: |
              (
                tenant:mongodb_connections_current:max
                > on(tenant) group_left
                tenant:alert_threshold:mongodb_connections_current
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "MongoDB high connection count on {{ $labels.tenant }}"
              description: "{{ $value }} current connections"

          - alert: MongoDBReplicationLag
            expr: |
              (
                tenant:mongodb_replication_lag:max
                > on(tenant) group_left
                tenant:alert_threshold:mongodb_replication_lag
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "MongoDB replication lag on {{ $labels.tenant }}"
              description: "Replication lag: {{ $value | printf \"%.1f\" }}s behind primary"

          - alert: MongoDBHighOperations
            expr: |
              (
                tenant:mongodb_opcounters:rate5m
                > on(tenant) group_left
                tenant:alert_threshold:mongodb_opcounters_rate
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "MongoDB high operation rate on {{ $labels.tenant }}"
              description: "{{ $value | printf \"%.0f\" }} ops/sec"

          - alert: MongoDBHighPageFaults
            expr: |
              tenant:mongodb_page_faults:rate5m > 100
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "MongoDB high page faults on {{ $labels.tenant }}"
              description: "{{ $value | printf \"%.1f\" }} page faults/sec — check memory"

          - alert: MongoDBConnectionSaturation
            expr: |
              tenant:mongodb_connection_usage:ratio > 0.8
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "MongoDB connection pool near saturation on {{ $labels.tenant }}"
              description: "{{ $value | printf \"%.0f\" }}% of connections used"

      # -------------------------------------------------------
      # Elasticsearch alerts (from rule-pack-elasticsearch.yaml)
      # Pre-loaded: no-metric → no-alert (near-zero cost)
      # -------------------------------------------------------
      - name: elasticsearch-alerts
        rules:
          - alert: ElasticsearchClusterRed
            expr: |
              tenant:es_cluster_health:status == 2
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "Elasticsearch cluster RED on {{ $labels.tenant }}"
              description: "Cluster health is RED — data loss or unavailability possible"

          - alert: ElasticsearchClusterYellow
            expr: |
              tenant:es_cluster_health:status == 1
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch cluster YELLOW on {{ $labels.tenant }}"
              description: "Some replica shards are unassigned"

          - alert: ElasticsearchHighHeapUsage
            expr: |
              (
                tenant:es_heap_usage_percent:max
                > on(tenant) group_left
                tenant:alert_threshold:es_heap_usage_percent
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch high heap usage on {{ $labels.tenant }}"
              description: "JVM heap: {{ $value | printf \"%.1f\" }}%"

          - alert: ElasticsearchHighDiskUsage
            expr: |
              (
                tenant:es_disk_usage_percent:max
                > on(tenant) group_left
                tenant:alert_threshold:es_disk_usage_percent
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch high disk usage on {{ $labels.tenant }}"
              description: "Disk usage: {{ $value | printf \"%.1f\" }}%"

          - alert: ElasticsearchHighSearchLatency
            expr: |
              (
                tenant:es_search_latency_ms:avg
                > on(tenant) group_left
                tenant:alert_threshold:es_search_latency_ms
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch high search latency on {{ $labels.tenant }}"
              description: "Average search latency: {{ $value | printf \"%.0f\" }}ms"

          - alert: ElasticsearchUnassignedShards
            expr: |
              tenant:es_unassigned_shards:count > 0
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch has unassigned shards on {{ $labels.tenant }}"
              description: "{{ $value }} unassigned shards — check node capacity"

          - alert: ElasticsearchPendingTasks
            expr: |
              (
                tenant:es_pending_tasks:max
                > on(tenant) group_left
                tenant:alert_threshold:es_pending_tasks
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch pending cluster tasks on {{ $labels.tenant }}"
              description: "{{ $value }} pending tasks — cluster may be overloaded"
