apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    alerting:
      alertmanagers:
        - static_configs:
            - targets:
                - "alertmanager.monitoring.svc.cluster.local:9093"

    rule_files:
      - "/etc/prometheus/rules/*.yml"

    scrape_configs:
      # -------------------------------------------------------
      # Self-monitoring (static)
      # -------------------------------------------------------
      - job_name: "prometheus"
        static_configs:
          - targets: ["localhost:9090"]

      # -------------------------------------------------------
      # Annotation-based Service Discovery
      # Any Service with prometheus.io/scrape: "true" is auto-discovered.
      # New tenants or components no longer require Prometheus config changes.
      # -------------------------------------------------------

      # Tenant DB exporters (db-a, db-b, db-c, ...)
      # Discovers services in db-* namespaces.
      # 'tenant' label is derived from the namespace.
      - job_name: "tenant-exporters"
        scrape_interval: 10s
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          # Only keep services with prometheus.io/scrape annotation
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: "true"
          # Only keep services in db-* namespaces
          - source_labels: [__meta_kubernetes_namespace]
            action: keep
            regex: "db-.+"
          # Use the annotated port
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          # Use annotated metrics path
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          # Set tenant label from namespace
          - source_labels: [__meta_kubernetes_namespace]
            target_label: tenant
          # Set instance label from namespace (backward compat)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: instance
          # Add env label
          - target_label: env
            replacement: "test"

      # Monitoring namespace services
      # Auto-discovers: kube-state-metrics, threshold-exporter,
      # and any future component with the annotation.
      - job_name: "monitoring-components"
        scrape_interval: 15s
        kubernetes_sd_configs:
          - role: service
            namespaces:
              names: ["monitoring"]
        relabel_configs:
          # Only keep annotated services
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: "true"
          # Use the annotated port
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          # Use annotated metrics path
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          # Set job label from service name for clarity
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job

      # -------------------------------------------------------
      # Scenario B: kubelet cAdvisor metrics (container-level CPU/memory)
      # Uses K8s API proxy to avoid direct kubelet TLS issues in Kind.
      # Provides: container_cpu_usage_seconds_total, container_memory_working_set_bytes
      # -------------------------------------------------------
      - job_name: "kubelet-cadvisor"
        scrape_interval: 15s
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
        metric_relabel_configs:
          # Only keep container metrics from tenant namespaces
          - source_labels: [namespace]
            action: keep
            regex: "db-.+"
          # Drop high-cardinality metrics we don't need
          - source_labels: [__name__]
            action: keep
            regex: "container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_memory_usage_bytes"

  # ============================================================
  # Recording Rules (Normalization Layer)
  # Canonical sources: rule-packs/rule-pack-kubernetes.yaml
  #                    rule-packs/rule-pack-mariadb.yaml
  # Optional packs:   rule-packs/rule-pack-redis/mongodb/elasticsearch.yaml
  # ============================================================
  recording-rules.yml: |
    groups:
      # -------------------------------------------------------
      # MariaDB normalization (from rule-pack-mariadb.yaml)
      # -------------------------------------------------------
      - name: mariadb-normalization
        interval: 15s
        rules:
          - record: tenant:mysql_cpu_usage:rate5m
            expr: sum by(tenant) (rate(mysql_global_status_threads_running[5m]))

          - record: tenant:mysql_threads_connected:sum
            expr: sum by(tenant) (mysql_global_status_threads_connected)

          - record: tenant:mysql_connection_usage:ratio
            expr: |
              sum by(tenant) (mysql_global_status_threads_connected)
              / sum by(tenant) (mysql_global_variables_max_connections)

          - record: tenant:mysql_uptime:hours
            expr: min by(tenant) (mysql_global_status_uptime) / 3600

          # Percona-inspired additional normalization
          - record: tenant:mysql_slow_queries:rate5m
            expr: sum by(tenant) (rate(mysql_global_status_slow_queries[5m]))

          - record: tenant:mysql_aborted_connections:rate5m
            expr: sum by(tenant) (rate(mysql_global_status_aborted_connects[5m]))

          - record: tenant:mysql_buffer_pool_usage:ratio
            expr: |
              sum by(tenant) (mysql_global_status_innodb_buffer_pool_pages_data)
              / sum by(tenant) (mysql_global_status_innodb_buffer_pool_pages_total)

      # -------------------------------------------------------
      # MariaDB threshold normalization (from rule-pack-mariadb.yaml)
      # -------------------------------------------------------
      - name: mariadb-threshold-normalization
        interval: 15s
        rules:
          - record: tenant:alert_threshold:connections
            expr: sum by(tenant) (user_threshold{metric="connections", severity="warning"})

          - record: tenant:alert_threshold:cpu
            expr: sum by(tenant) (user_threshold{metric="cpu", severity="warning"})

          # Multi-tier severity (Scenario D)
          - record: tenant:alert_threshold:connections_critical
            expr: sum by(tenant) (user_threshold{metric="connections", severity="critical"})

          - record: tenant:alert_threshold:cpu_critical
            expr: sum by(tenant) (user_threshold{metric="cpu", severity="critical"})

      # -------------------------------------------------------
      # Kubernetes container normalization (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-container-normalization
        interval: 15s
        rules:
          # Container CPU usage as % of limit
          - record: tenant:container_cpu_percent:by_container
            expr: |
              label_replace(
                sum by(namespace, pod, container) (
                  rate(container_cpu_usage_seconds_total{namespace=~"db-.+", container!="", container!="POD"}[5m])
                )
                /
                sum by(namespace, pod, container) (
                  kube_pod_container_resource_limits{resource="cpu", namespace=~"db-.+"}
                )
                * 100,
                "tenant", "$1", "namespace", "(.*)"
              )

          # Container memory usage as % of limit
          - record: tenant:container_memory_percent:by_container
            expr: |
              label_replace(
                sum by(namespace, pod, container) (
                  container_memory_working_set_bytes{namespace=~"db-.+", container!="", container!="POD"}
                )
                /
                sum by(namespace, pod, container) (
                  kube_pod_container_resource_limits{resource="memory", namespace=~"db-.+"}
                )
                * 100,
                "tenant", "$1", "namespace", "(.*)"
              )

          # Weakest link: MAX across containers per pod
          - record: tenant:pod_weakest_cpu_percent:max
            expr: max by(tenant, pod) (tenant:container_cpu_percent:by_container)

          - record: tenant:pod_weakest_memory_percent:max
            expr: max by(tenant, pod) (tenant:container_memory_percent:by_container)

      # -------------------------------------------------------
      # Kubernetes state matching (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-state-matching
        interval: 15s
        rules:
          - record: tenant:container_waiting_reason:count
            expr: |
              label_replace(
                count by(namespace, reason) (
                  kube_pod_container_status_waiting_reason{namespace=~"db-.+"} > 0
                ),
                "tenant", "$1", "namespace", "(.*)"
              )

      # -------------------------------------------------------
      # Kubernetes threshold normalization (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-threshold-normalization
        interval: 15s
        rules:
          - record: tenant:alert_threshold:container_cpu
            expr: sum by(tenant) (user_threshold{component="container", metric="cpu"})

          - record: tenant:alert_threshold:container_memory
            expr: sum by(tenant) (user_threshold{component="container", metric="memory"})

  # ============================================================
  # Alert Rules (using normalized metrics)
  # Canonical sources: rule-packs/rule-pack-mariadb.yaml
  #                    rule-packs/rule-pack-kubernetes.yaml
  # Optional packs:   rule-packs/rule-pack-redis/mongodb/elasticsearch.yaml
  # ============================================================
  alert-rules.yml: |
    groups:
      # -------------------------------------------------------
      # MariaDB alerts (from rule-pack-mariadb.yaml)
      # -------------------------------------------------------
      - name: mariadb-alerts
        rules:
          - alert: MariaDBDown
            expr: mysql_up == 0
            for: 15s
            labels:
              severity: critical
            annotations:
              summary: "MariaDB instance {{ $labels.instance }} is DOWN"
              description: "mysql_up=0 for 15s on {{ $labels.instance }}"

          - alert: MariaDBExporterAbsent
            expr: absent(mysql_up{job="tenant-exporters"})
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "mysqld-exporter target missing"
              description: "No mysql_up metric found for 30s"

          - alert: MariaDBHighConnections
            expr: |
              (
                tenant:mysql_threads_connected:sum
                > on(tenant) group_left
                tenant:alert_threshold:connections
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
              unless on(tenant)
              (
                tenant:mysql_threads_connected:sum
                > on(tenant) group_left
                tenant:alert_threshold:connections_critical
              )
            for: 30s
            labels:
              severity: warning
            annotations:
              summary: "High connections on {{ $labels.tenant }}"
              description: "{{ $value }} threads connected (warning threshold exceeded)"

          # Critical tier â€” suppresses warning via unless above
          - alert: MariaDBHighConnectionsCritical
            expr: |
              (
                tenant:mysql_threads_connected:sum
                > on(tenant) group_left
                tenant:alert_threshold:connections_critical
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "Critical connections on {{ $labels.tenant }}"
              description: "{{ $value }} threads connected (critical threshold exceeded)"

          # Composite alert: connections AND CPU both exceeded
          - alert: MariaDBSystemBottleneck
            expr: |
              (
                (
                  tenant:mysql_threads_connected:sum
                  > on(tenant) group_left
                  tenant:alert_threshold:connections
                )
                and on(tenant)
                (
                  tenant:mysql_cpu_usage:rate5m
                  > on(tenant) group_left
                  tenant:alert_threshold:cpu
                )
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 60s
            labels:
              severity: critical
            annotations:
              summary: "System bottleneck on {{ $labels.tenant }}"
              description: "Both connections AND CPU thresholds exceeded simultaneously"

          - alert: MariaDBRecentRestart
            expr: mysql_global_status_uptime < 300
            for: 0s
            labels:
              severity: info
            annotations:
              summary: "{{ $labels.instance }} restarted recently"
              description: "Uptime is only {{ $value }}s (< 5 min)"

          # Percona-inspired: High slow queries rate
          - alert: MariaDBHighSlowQueries
            expr: |
              tenant:mysql_slow_queries:rate5m > 1
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High slow query rate on {{ $labels.tenant }}"
              description: "{{ $value | printf \"%.2f\" }} slow queries/sec"

          # Percona-inspired: High aborted connections
          - alert: MariaDBHighAbortedConnections
            expr: |
              tenant:mysql_aborted_connections:rate5m > 5
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High aborted connections on {{ $labels.tenant }}"
              description: "{{ $value | printf \"%.2f\" }} aborted connections/sec"

      # -------------------------------------------------------
      # Kubernetes container alerts (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-container-alerts
        rules:
          - alert: PodContainerHighCPU
            expr: |
              (
                max by(tenant) (tenant:pod_weakest_cpu_percent:max)
                > on(tenant) group_left
                tenant:alert_threshold:container_cpu
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 60s
            labels:
              severity: warning
            annotations:
              summary: "High container CPU in {{ $labels.tenant }}"
              description: "Max container CPU usage: {{ $value | printf \"%.1f\" }}% (threshold exceeded)"

          - alert: PodContainerHighMemory
            expr: |
              (
                max by(tenant) (tenant:pod_weakest_memory_percent:max)
                > on(tenant) group_left
                tenant:alert_threshold:container_memory
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 60s
            labels:
              severity: warning
            annotations:
              summary: "High container memory in {{ $labels.tenant }}"
              description: "Max container memory usage: {{ $value | printf \"%.1f\" }}% (threshold exceeded)"

      # -------------------------------------------------------
      # Kubernetes pod state alerts (from rule-pack-kubernetes.yaml)
      # -------------------------------------------------------
      - name: kubernetes-pod-state-alerts
        rules:
          - alert: ContainerCrashLoop
            expr: |
              (
                (
                  tenant:container_waiting_reason:count{reason="CrashLoopBackOff"}
                  * on(tenant) group_left
                  user_state_filter{filter="container_crashloop"}
                ) > 0
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "Container crash loop detected in {{ $labels.tenant }}"
              description: "{{ $value }} container(s) in CrashLoopBackOff state"

          - alert: ContainerImagePullFailure
            expr: |
              (
                (
                  tenant:container_waiting_reason:count{reason=~"ImagePullBackOff|InvalidImageName"}
                  * on(tenant) group_left
                  user_state_filter{filter="container_imagepull"}
                ) > 0
              )
              unless on(tenant)
              (user_state_filter{filter="maintenance"} == 1)
            for: 30s
            labels:
              severity: warning
            annotations:
              summary: "Image pull failure detected in {{ $labels.tenant }}"
              description: "{{ $value }} container(s) in ImagePullBackOff/InvalidImageName state"
